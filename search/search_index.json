{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p><code>cgcnn2</code> is a reproduction of crystal graph convolutional neural network (CGCNN) for predicting material properties. This documentation will help you get started with using CGCNN for your materials science research.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Training a CGCNN model using a custom dataset.</li> <li>Predicting material properties with a pre-trained CGCNN model.</li> <li>Fine-tuning a pre-trained CGCNN model on a new dataset.</li> <li>Extracting structural features as descriptors for downstream tasks.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Package Installation</li> <li>CGCNN Preliminaries</li> <li>Function Usage</li> <li>Examples</li> </ol>"},{"location":"1_installation/","title":"Installation","text":""},{"location":"1_installation/#prerequisite","title":"Prerequisite","text":"<ul> <li>Python 3.11 or higher</li> </ul>"},{"location":"1_installation/#installation-steps","title":"Installation Steps","text":"<p>It is recommended to first check your available CUDA version (or CPU only) and install PyTorch following the instructions on the PyTorch official website. Then, you can simply install <code>cgcnn2</code> from PyPI using pip:</p> <pre><code>pip install cgcnn2\n</code></pre> <p>If you'd like to use the latest unreleased version on the main branch, you can install it directly from GitHub:</p> <pre><code>pip install git+https://github.com/jcwang587/cgcnn2@main\n</code></pre>"},{"location":"1_installation/#dependencies","title":"Dependencies","text":"<p>The package requires the following dependencies:</p> <ul> <li>matplotlib</li> <li>NumPy</li> <li>pandas</li> <li>pymatgen</li> <li>PyTorch</li> </ul> <p>These will be automatically installed when you install the package.</p>"},{"location":"1_installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify your installation, launch the Python interpreter and run:</p> <pre><code>import cgcnn2\n\nprint(cgcnn2.__version__)\n</code></pre>"},{"location":"2_cgcnn/","title":"CGCNN","text":""},{"location":"2_cgcnn/#crystal-graph-convolutional-neural-network-cgcnn","title":"Crystal Graph Convolutional Neural Network (CGCNN)","text":""},{"location":"2_cgcnn/#introduction","title":"Introduction","text":"<p>The Crystal Graph Convolutional Neural Network (CGCNN) is a deep learning framework designed for predicting material properties based on their crystal structures. It was introduced in the paper \"Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties\".</p>"},{"location":"2_cgcnn/#graph-representation","title":"Graph Representation","text":"<p>The main idea in CGCNN is to represent the crystal structure by a crystal graph that encodes both atomic information and bonding interactions between atoms. A crystal graph \\(\\mathcal{G}\\) is an undirected multigraph which is defined by nodes representing atoms and edges representing connections between atoms in a crystal.</p> <p>Each node \\(i\\) is represented by a feature vector \\(v_i\\), encoding the property of the atom corresponding to node \\(i\\). Similarly, each edge \\((i,j)_k\\) is represented by a feature vector \\(u_{(i,j)_k}\\) corresponding to the \\(k\\)th bond connecting atom \\(i\\) and atom \\(j\\).</p> <p>The crystal graph is unlike normal graphs since it allows multiple edges between the same pair of end nodes, a characteristic for crystal graphs due to their periodicity, in contrast to molecular graphs.</p> <p> </p> <pre><code>graph LR\n    A[Na1] --- B[Cl1]\n    A --- B\n    A --- C[Cl2]\n    A --- C\n    A --- E[Cl3]\n    A --- E\n    B --- D[Na2]\n    B --- D\n    B --- F[Na3]\n    B --- F\n    C --- D\n    C --- D\n    C --- G[Na4]\n    C --- G\n    D --- H[Cl4]\n    D --- H\n    E --- F\n    E --- F\n    E --- G\n    E --- G\n    F --- H\n    F --- H\n    G --- H\n    G --- H</code></pre>"},{"location":"2_cgcnn/#model-architecture","title":"Model Architecture","text":""},{"location":"2_cgcnn/#graph-neural-network","title":"Graph Neural Network","text":"<p>The convolutional neural networks built on top of the crystal graph consist of two major components: convolutional layers and pooling layers. The convolutional layers iteratively update the atom feature vector \\(v_i\\) by \"convolution\" with surrounding atoms and bonds with a nonlinear graph convolution function,</p> \\[ v_i^{(t+1)} = \\text{Conv}\\left(v_i^{(t)}, v_j^{(t)}, \\mathbf{u}_{(i,j)_k}\\right), \\quad (i,j)_k \\in \\mathcal{G}. \\tag{1} \\] <p>After \\(R\\) convolutions, the network automatically learns the feature vector \\(v_i^{(R)}\\) for each atom by iteratively including its surrounding environment. The pooling layer is then used for producing an overall feature vector \\(v_c\\) for the crystal, which can be represented by a pooling function,</p> \\[ v_c = \\text{Pool}(v_0^{(0)}, v_1^{(0)}, \\ldots, v_N^{(0)}, \\ldots, v_N^{(R)}) \\tag{2} \\] <p>that is invariant under permutations of atom indices and independent of the choice of unit cell. In this work, a normalized summation is used as the pooling function for simplicity, but other functions can also be used. In addition to the convolutional and pooling layers, two fully connected hidden layers with the depths of \\(L_1\\) and \\(L_2\\) are added to capture the complex mapping between crystal structure and property. Finally, an output layer is used to connect the \\(L_2\\) hidden layer to predict the target property \\(\\hat{y}\\).</p> <pre><code>graph LR\n    A[Embed] --&gt; B[Conv] --&gt; C[Pool] --&gt; D[L1] --&gt; |softplus| E[L2] --&gt; F[Out]\n    E --&gt; |softmax| G[Out]</code></pre>"},{"location":"2_cgcnn/#convolutional-layer","title":"Convolutional Layer","text":"<p>The convolutional operation in CGCNN can be expressed as:</p> \\[ v_i^{(t+1)} = v_i^{(t)} + \\sum_{j,k} \\sigma\\left(z_{(i,j)_k}^{(t)} W_f^{(t)} + b_f^{(t)}\\right) \\odot g\\left(z_{(i,j)_k}^{(t)} W_s^{(t)} + b_s^{(t)}\\right) \\tag{3} \\] \\[ z_{(i,j)_k}^{(t)} = v_i^{(t)} \\oplus v_j^{(t)} \\oplus u_{(i,j)_k} \\tag{4} \\] <p>where:</p> <ul> <li>\\(v_i^{(t)}\\) is the feature vector of atom \\(i\\) at layer \\(t\\)</li> <li>\\(\\sigma\\) is the sigmoid function (gate)</li> <li>\\(g\\) is the softplus activation function for introducing nonlinear coupling between layers</li> <li>\\(\\odot\\) is the element-wise multiplication</li> <li>\\(\\oplus\\) is the concatenation operation</li> <li>\\(z_{(i,j)_k}^{(t)}\\) is the concatenation of the feature vectors of atom \\(i\\), atom \\(j\\), and the \\(k\\)th bond between atom \\(i\\) and atom \\(j\\) at layer \\(t\\)</li> <li>\\(W_f^{(t)}\\) and \\(b_f^{(t)}\\) are the learnable weights and biases for the sigmoid function</li> <li>\\(W_s^{(t)}\\) and \\(b_s^{(t)}\\) are the learnable weights and biases for the softplus function</li> </ul>"},{"location":"2_cgcnn/#references","title":"References","text":"<ol> <li>Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties</li> <li>CGCNN GitHub Repository</li> </ol>"},{"location":"3_usage/","title":"Usage","text":""},{"location":"3_usage/#basic-usage","title":"Basic Usage","text":"<p>Here's a basic tutorial on going through the prediction script using the functions provided by the package.</p>"},{"location":"3_usage/#1-importing-the-package","title":"1. Importing the package","text":"<p>There are three main modules available in the package:</p> <ul> <li><code>cgcnn2.data</code>: For loading and preprocessing the data.</li> <li><code>cgcnn2.model</code>: Building blocks for the CGCNN model.</li> <li><code>cgcnn2.utils</code>: Some utility functions.</li> </ul> <pre><code>from cgcnn2.data import CIFData, collate_pool\nfrom cgcnn2.model import CrystalGraphConvNet\nfrom cgcnn2.utils import cgcnn_test\n</code></pre>"},{"location":"3_usage/#2-data-preparation","title":"2. Data Preparation","text":"<p>To input material structures into CGCNN, you need to define a custom dataset. Before doing so, make sure you have the following files:</p> <ul> <li><code>CIF</code> files recording the structures of the materials you wish to study.</li> <li>Target properties for each material (not needed for prediction jobs).</li> </ul> <p>Organize these files in a directory (<code>root_dir</code>) with the following structure:</p> <ol> <li><code>id_prop.csv</code> (optional for prediction):    A CSV with two columns, the first column is a unique material ID, and the second column is the corresponding target property value.</li> <li><code>atom_init.json</code>:    A <code>JSON</code> file that provides the initialization vector for each element. You can use the example at <code>/cgcnn2/asset/atom_init.json</code> from the original CGCNN repository; it should work for most applications.</li> <li><code>CIF</code> files:    One <code>.cif</code> file per material, named <code>ID.cif</code>, where <code>ID</code> matches the entries in <code>id_prop.csv</code>.</li> </ol> <p>Once your <code>root_dir</code> (for example, <code>/examples/data/sample_regression</code>) contains these files, you can load the dataset using the <code>CIFData</code> class:</p> <pre><code>dataset = CIFData(\"/examples/data/sample_regression\")\n</code></pre> <p>This will prepare your crystal structures (and, if provided, their target properties) for use with CGCNN. Then, we can build a <code>torch.utils.data.DataLoader</code> object that can be used to load the dataset in a batch.</p> <pre><code>from torch.utils.data import DataLoader\n\nloader = DataLoader(\n    dataset,\n    batch_size=args.batch_size,\n    shuffle=False,\n    num_workers=args.workers,\n    collate_fn=collate_pool,\n)\n</code></pre>"},{"location":"3_usage/#3-model-initialization","title":"3. Model Initialization","text":"<p>We need some information from the dataset to initialize the model, which can be done by:</p> <pre><code>atom_graph, _, _ = dataset[0]\norig_atom_fea_len = atom_graph[0].shape[-1]\nnbr_fea_len = atom_graph[1].shape[-1]\n</code></pre> <p>where <code>dataset[0]</code> is a tuple of <code>(atom_graph, target, cif_id)</code>, where <code>atom_graph</code> is a tuple of <code>(atom_fea, nbr_fea, nbr_fea_idx)</code>, <code>target</code> is the target property value, and <code>cif_id</code> is the unique ID of the material. The <code>atom_graph</code> tuple contains the atom features, neighbor features, and neighbor indices, and the dimensions of these features given by <code>orig_atom_fea_len</code> and <code>nbr_fea_len</code> are needed to initialize the model.</p> <p>Besides, we need some information about the pre-trained model architecture, which can be done by:</p> <pre><code>import torch\nimport argparse\n\ncheckpoint = torch.load(args.model_path, map_location=args.device)\nmodel_args = argparse.Namespace(**checkpoint[\"args\"])\natom_fea_len = model_args.atom_fea_len\nn_conv = model_args.n_conv\nh_fea_len = model_args.h_fea_len\nn_h = model_args.n_h\n</code></pre> <p>where <code>atom_fea_len</code>, <code>n_conv</code>, <code>h_fea_len</code>, and <code>n_h</code> are the dimensions of the atom features, the number of convolutional layers, the dimension of the hidden features, and the number of hidden layers, respectively. Now, we can initialize the model by:</p> <pre><code>model = CrystalGraphConvNet(\n    orig_atom_fea_len=orig_atom_fea_len,\n    nbr_fea_len=nbr_fea_len,\n    atom_fea_len=atom_fea_len,\n    n_conv=n_conv,\n    h_fea_len=h_fea_len,\n    n_h=n_h,\n)\n</code></pre>"},{"location":"3_usage/#4-model-loading-and-prediction","title":"4. Model Loading and Prediction","text":"<p>The <code>checkpoint</code> from the pre-trained model includes the model's state dictionary and training arguments. The state dictionary contains all the learned parameters of the model, while the training arguments store the hyperparameters used during training. The model can be loaded onto either CPU or GPU device by specifying <code>device</code> as <code>cpu</code> or <code>cuda</code>. Using GPU is recommended for faster inference if available.</p> <pre><code>model.load_state_dict(checkpoint[\"state_dict\"])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n</code></pre> <p>Now, we can run the prediction with the <code>cgcnn_test</code> utility function:</p> <pre><code>cgcnn_test(\n    model=model,\n    loader=loader,\n    device=device,\n    plot_file=os.path.join(output_folder, \"parity_plot.svg\"),\n)\n</code></pre>"},{"location":"3_usage/#training-options","title":"Training Options","text":"<p>You can view the training hyperparameters by using the <code>--help</code> flag with both the <code>cgcnn-tr</code> and <code>cgcnn-ft</code> commands. Most of the hyperparameters are shared between these two scripts, including:</p> <ul> <li> <p><code>batch-size</code>: Specifies the number of samples that are grouped together and processed in one forward/backward pass during both training and fine-tuning. At each step, the DataLoader will pull <code>batch-size</code> examples from the dataset, feed them through the model, compute the loss, and then perform a gradient update. Choosing the right <code>batch-size</code> involves a trade-off between memory usage, computational efficiency, and optimization dynamics.   By default, we set <code>batch-size = 256</code> as a balance between speed and memory requirements on modern GPUs. If you run into OOM errors, try reducing this value; if you have excess memory and want to speed up training, experiment with increasing it.</p> </li> <li> <p><code>learning-rate</code>: Determines the step size used by the optimizer to update model weights at each gradient step. A higher learning rate makes larger jumps in parameter space, potentially speeding up initial convergence but risking instability or divergence. A lower rate yields more precise, stable updates at the cost of slower training. By default, we set learning-rate = 1e-2 as a good starting point for most graph-based models. If loss oscillates or diverges, try reducing it; if training stalls, consider increasing it or adding a scheduler.</p> </li> <li> <p><code>epoch</code>: Specifies how many epochs the training loop will make over the entire dataset during both training and fine-tuning. More epochs give the model more opportunities to learn but increase total runtime and can lead to overfitting if too many are used. The default is <code>epoch = 1000</code>. Monitor your validation loss and use early stopping if you observe that performance plateaus.</p> </li> <li> <p><code>device</code>: Indicates which hardware backend to use for both data loading and model computation. Usually <code>cuda</code> for NVIDIA GPUs or <code>cpu</code>. When set to <code>cuda</code>, tensors and the model are moved onto the GPU. By default, we auto-detect the device by checking if <code>cuda</code> is available. Using the GPU generally yields significant speed-ups.</p> </li> </ul> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"3_usage/#early-stopping","title":"Early Stopping","text":"<p>Both the training and fine-tuning scripts implement an early stopping strategy that halts training if the validation loss fails to improve for a specified number of consecutive epochs.</p> <ul> <li><code>stop-patience</code>: Defines how many consecutive epochs without any decrease in validation loss are allowed before training is terminated. Default is <code>None</code>, which means no early stopping.</li> </ul>"},{"location":"3_usage/#learning-rate-scheduler","title":"Learning Rate Scheduler","text":"<p>There is a learning rate scheduler for the training and finetuning scripts, which applies the <code>ReduceLROnPlateau</code> strategy. The default parameters are:</p> <ul> <li><code>factor</code>: The factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code></li> <li><code>patience</code>: How many epochs to wait before reducing the learning rate.</li> </ul> <p>You can check more details in the PyTorch documentation.</p>"},{"location":"4_examples/","title":"Examples","text":""},{"location":"4_examples/#programmatic-examples","title":"Programmatic Examples","text":"<p>A series of notebooks demonstrates how to load and use the pretrained models for property prediction and structural descriptor extraction.</p> <ol> <li>Property Prediction</li> <li>Structural Descriptor</li> </ol>"},{"location":"4_examples/#command-line-examples","title":"Command Line Examples","text":"<p>The following Slurm scripts are examples of how to run the training, prediction, and fine-tuning of the models.</p> <ol> <li>Train a CGCNN from scratch</li> <li>Make predictions using a pretrained model</li> <li>Fine-tune a pretrained model</li> </ol>"},{"location":"5_api_reference/","title":"API Reference","text":""},{"location":"5_api_reference/#cgcnn2.data","title":"<code>cgcnn2.data</code>","text":""},{"location":"5_api_reference/#cgcnn2.data.AtomCustomJSONInitializer","title":"<code>AtomCustomJSONInitializer</code>","text":"<p>               Bases: <code>AtomInitializer</code></p> <p>Initialize atom feature vectors using a JSON file, which is a python dictionary mapping from element number to a list representing the feature vector of the element.</p> <p>Parameters:</p> Name Type Description Default <code>elem_embedding_file</code> <code>str</code> <p>The path to the <code>.json</code> file</p> required Source code in <code>cgcnn2/data.py</code> <pre><code>class AtomCustomJSONInitializer(AtomInitializer):\n    \"\"\"\n    Initialize atom feature vectors using a JSON file, which is a python\n    dictionary mapping from element number to a list representing the\n    feature vector of the element.\n\n    Args:\n        elem_embedding_file (str): The path to the `.json` file\n    \"\"\"\n\n    def __init__(self, elem_embedding_file):\n        \"\"\"\n        Initialize atom feature embeddings from a JSON file mapping element numbers to feature vectors.\n\n        Parameters:\n            elem_embedding_file (str): Path to a JSON file where keys are element numbers and values are feature vectors.\n        \"\"\"\n        with open(elem_embedding_file) as f:\n            elem_embedding = json.load(f)\n        elem_embedding = {int(key): value for key, value in elem_embedding.items()}\n        atom_types = set(elem_embedding.keys())\n        super().__init__(atom_types)\n        for key, value in elem_embedding.items():\n            self._embedding[key] = np.array(value, dtype=float)\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.AtomCustomJSONInitializer.__init__","title":"<code>__init__(elem_embedding_file)</code>","text":"<p>Initialize atom feature embeddings from a JSON file mapping element numbers to feature vectors.</p> <p>Parameters:</p> Name Type Description Default <code>elem_embedding_file</code> <code>str</code> <p>Path to a JSON file where keys are element numbers and values are feature vectors.</p> required Source code in <code>cgcnn2/data.py</code> <pre><code>def __init__(self, elem_embedding_file):\n    \"\"\"\n    Initialize atom feature embeddings from a JSON file mapping element numbers to feature vectors.\n\n    Parameters:\n        elem_embedding_file (str): Path to a JSON file where keys are element numbers and values are feature vectors.\n    \"\"\"\n    with open(elem_embedding_file) as f:\n        elem_embedding = json.load(f)\n    elem_embedding = {int(key): value for key, value in elem_embedding.items()}\n    atom_types = set(elem_embedding.keys())\n    super().__init__(atom_types)\n    for key, value in elem_embedding.items():\n        self._embedding[key] = np.array(value, dtype=float)\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.AtomInitializer","title":"<code>AtomInitializer</code>","text":"<p>Base class for initializing the vector representation for atoms. Use one <code>AtomInitializer</code> per dataset.</p> Source code in <code>cgcnn2/data.py</code> <pre><code>class AtomInitializer:\n    \"\"\"\n    Base class for initializing the vector representation for atoms.\n    Use one `AtomInitializer` per dataset.\n    \"\"\"\n\n    def __init__(self, atom_types):\n        \"\"\"\n        Initialize the atom types and embedding dictionary.\n\n        Args:\n            atom_types (set): A set of unique atom types in the dataset.\n        \"\"\"\n        self.atom_types = set(atom_types)\n        self._embedding = {}\n\n    def get_atom_fea(self, atom_type):\n        \"\"\"\n        Get the vector representation for an atom type.\n\n        Args:\n            atom_type (str): The type of atom to get the vector representation for.\n        \"\"\"\n        assert atom_type in self.atom_types\n        return self._embedding[atom_type]\n\n    def load_state_dict(self, state_dict):\n        \"\"\"\n        Load the state dictionary for the atom initializer.\n\n        Args:\n            state_dict (dict): The state dictionary to load.\n        \"\"\"\n        self._embedding = state_dict\n        self.atom_types = set(self._embedding.keys())\n        self._decodedict = {\n            idx: atom_type for atom_type, idx in self._embedding.items()\n        }\n\n    def state_dict(self) -&gt; dict:\n        \"\"\"\n        Get the state dictionary for the atom initializer.\n\n        Returns:\n            dict: The state dictionary.\n        \"\"\"\n        return self._embedding\n\n    def decode(self, idx: int) -&gt; str:\n        \"\"\"\n        Decode an index to an atom type.\n\n        Args:\n            idx (int): The index to decode.\n\n        Returns:\n            str: The decoded atom type.\n        \"\"\"\n        if not hasattr(self, \"_decodedict\"):\n            self._decodedict = {\n                idx: atom_type for atom_type, idx in self._embedding.items()\n            }\n        return self._decodedict[idx]\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.AtomInitializer.__init__","title":"<code>__init__(atom_types)</code>","text":"<p>Initialize the atom types and embedding dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>atom_types</code> <code>set</code> <p>A set of unique atom types in the dataset.</p> required Source code in <code>cgcnn2/data.py</code> <pre><code>def __init__(self, atom_types):\n    \"\"\"\n    Initialize the atom types and embedding dictionary.\n\n    Args:\n        atom_types (set): A set of unique atom types in the dataset.\n    \"\"\"\n    self.atom_types = set(atom_types)\n    self._embedding = {}\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.AtomInitializer.decode","title":"<code>decode(idx)</code>","text":"<p>Decode an index to an atom type.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index to decode.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The decoded atom type.</p> Source code in <code>cgcnn2/data.py</code> <pre><code>def decode(self, idx: int) -&gt; str:\n    \"\"\"\n    Decode an index to an atom type.\n\n    Args:\n        idx (int): The index to decode.\n\n    Returns:\n        str: The decoded atom type.\n    \"\"\"\n    if not hasattr(self, \"_decodedict\"):\n        self._decodedict = {\n            idx: atom_type for atom_type, idx in self._embedding.items()\n        }\n    return self._decodedict[idx]\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.AtomInitializer.get_atom_fea","title":"<code>get_atom_fea(atom_type)</code>","text":"<p>Get the vector representation for an atom type.</p> <p>Parameters:</p> Name Type Description Default <code>atom_type</code> <code>str</code> <p>The type of atom to get the vector representation for.</p> required Source code in <code>cgcnn2/data.py</code> <pre><code>def get_atom_fea(self, atom_type):\n    \"\"\"\n    Get the vector representation for an atom type.\n\n    Args:\n        atom_type (str): The type of atom to get the vector representation for.\n    \"\"\"\n    assert atom_type in self.atom_types\n    return self._embedding[atom_type]\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.AtomInitializer.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Load the state dictionary for the atom initializer.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict</code> <p>The state dictionary to load.</p> required Source code in <code>cgcnn2/data.py</code> <pre><code>def load_state_dict(self, state_dict):\n    \"\"\"\n    Load the state dictionary for the atom initializer.\n\n    Args:\n        state_dict (dict): The state dictionary to load.\n    \"\"\"\n    self._embedding = state_dict\n    self.atom_types = set(self._embedding.keys())\n    self._decodedict = {\n        idx: atom_type for atom_type, idx in self._embedding.items()\n    }\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.AtomInitializer.state_dict","title":"<code>state_dict()</code>","text":"<p>Get the state dictionary for the atom initializer.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The state dictionary.</p> Source code in <code>cgcnn2/data.py</code> <pre><code>def state_dict(self) -&gt; dict:\n    \"\"\"\n    Get the state dictionary for the atom initializer.\n\n    Returns:\n        dict: The state dictionary.\n    \"\"\"\n    return self._embedding\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.CIFData","title":"<code>CIFData</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>The CIFData dataset is a wrapper for a dataset where the crystal structures are stored in the form of CIF files.</p> <p><code>id_prop.csv</code>: a CSV file with two columns. The first column records a unique ID for each crystal, and the second column records the value of target property.</p> <p><code>atom_init.json</code>: a JSON file that stores the initialization vector for each element.</p> <p><code>ID.cif</code>: a CIF file that records the crystal structure, where ID is the unique ID for the crystal.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the root directory of the dataset</p> required <code>max_num_nbr</code> <code>int</code> <p>The maximum number of neighbors while constructing the crystal graph</p> <code>12</code> <code>radius</code> <code>float</code> <p>The cutoff radius for searching neighbors</p> <code>8</code> <code>dmin</code> <code>float</code> <p>The minimum distance for constructing GaussianDistance</p> <code>0</code> <code>step</code> <code>float</code> <p>The step size for constructing GaussianDistance</p> <code>0.2</code> <code>cache_size</code> <code>int | None</code> <p>The size of the lru cache for the dataset. Default is None.</p> <code>None</code> <code>random_seed</code> <code>int</code> <p>Random seed for shuffling the dataset</p> <code>123</code> <p>Returns:</p> Name Type Description <code>atom_fea</code> <code>Tensor</code> <p>shape (n_i, atom_fea_len)</p> <code>nbr_fea</code> <code>Tensor</code> <p>shape (n_i, M, nbr_fea_len)</p> <code>nbr_fea_idx</code> <code>LongTensor</code> <p>shape (n_i, M)</p> <code>target</code> <code>Tensor</code> <p>shape (1, )</p> <code>cif_id</code> <code>str or int</code> <p>Unique ID for the crystal</p> Source code in <code>cgcnn2/data.py</code> <pre><code>class CIFData(Dataset):\n    \"\"\"\n    The CIFData dataset is a wrapper for a dataset where the crystal structures\n    are stored in the form of CIF files.\n\n    `id_prop.csv`: a CSV file with two columns. The first column records a\n    unique ID for each crystal, and the second column records the value of\n    target property.\n\n    `atom_init.json`: a JSON file that stores the initialization vector for each\n    element.\n\n    `ID.cif`: a CIF file that records the crystal structure, where ID is the\n    unique ID for the crystal.\n\n    Args:\n        root_dir (str): The path to the root directory of the dataset\n        max_num_nbr (int): The maximum number of neighbors while constructing the crystal graph\n        radius (float): The cutoff radius for searching neighbors\n        dmin (float): The minimum distance for constructing GaussianDistance\n        step (float): The step size for constructing GaussianDistance\n        cache_size (int | None): The size of the lru cache for the dataset. Default is None.\n        random_seed (int): Random seed for shuffling the dataset\n\n    Returns:\n        atom_fea (torch.Tensor): shape (n_i, atom_fea_len)\n        nbr_fea (torch.Tensor): shape (n_i, M, nbr_fea_len)\n        nbr_fea_idx (torch.LongTensor): shape (n_i, M)\n        target (torch.Tensor): shape (1, )\n        cif_id (str or int): Unique ID for the crystal\n    \"\"\"\n\n    def __init__(\n        self,\n        root_dir,\n        max_num_nbr=12,\n        radius=8,\n        dmin=0,\n        step=0.2,\n        cache_size=None,\n        random_seed=123,\n    ):\n        self.root_dir = root_dir\n        self.max_num_nbr, self.radius = max_num_nbr, radius\n        assert os.path.exists(root_dir), \"root_dir does not exist!\"\n        id_prop_file = os.path.join(self.root_dir, \"id_prop.csv\")\n        assert os.path.exists(id_prop_file), \"id_prop.csv does not exist!\"\n        with open(id_prop_file) as f:\n            reader = csv.reader(f)\n            self.id_prop_data = [row for row in reader]\n        random.seed(random_seed)\n        atom_init_file = os.path.join(self.root_dir, \"atom_init.json\")\n        assert os.path.exists(atom_init_file), \"atom_init.json does not exist!\"\n        self.ari = AtomCustomJSONInitializer(atom_init_file)\n        self.gdf = GaussianDistance(dmin=dmin, dmax=self.radius, step=step)\n        self._raw_load_item = self._load_item_fast\n        self.cache_size = cache_size\n        self._configure_cache()\n\n    def set_cache_size(self, cache_size: Optional[int]) -&gt; None:\n        \"\"\"\n        Change the LRU-cache capacity on the fly.\n\n        Args:\n            cache_size (int | None): The size of the cache to set, None for unlimited size. Default is None.\n        \"\"\"\n        self.cache_size = cache_size\n        if hasattr(self._cache_load, \"cache_clear\"):\n            self._cache_load.cache_clear()\n        self._configure_cache()\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"\n        Clear the current cache.\n        \"\"\"\n        if hasattr(self._cache_load, \"cache_clear\"):\n            self._cache_load.cache_clear()\n\n    def __len__(self):\n        return len(self.id_prop_data)\n\n    def __getitem__(self, idx):\n        return self._cache_load(idx)\n\n    def _configure_cache(self) -&gt; None:\n        \"\"\"\n        Wrap `_raw_load_item` with an LRU cache.\n        \"\"\"\n        if self.cache_size is None:\n            self._cache_load = functools.lru_cache(maxsize=None)(self._raw_load_item)\n        elif self.cache_size &lt;= 0:\n            self._cache_load = self._raw_load_item\n        else:\n            self._cache_load = functools.lru_cache(maxsize=self.cache_size)(\n                self._raw_load_item\n            )\n\n    def _load_item(self, idx):\n        cif_id, target = self.id_prop_data[idx]\n        crystal = Structure.from_file(os.path.join(self.root_dir, cif_id + \".cif\"))\n        atom_fea = np.vstack(\n            [\n                self.ari.get_atom_fea(crystal[i].specie.number)\n                for i in range(len(crystal))\n            ]\n        )\n        atom_fea = torch.Tensor(atom_fea)\n        all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)\n        all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]\n        nbr_fea_idx, nbr_fea = [], []\n        for nbr in all_nbrs:\n            if len(nbr) &lt; self.max_num_nbr:\n                warnings.warn(\n                    \"{} not find enough neighbors to build graph. \"\n                    \"If it happens frequently, consider increase \"\n                    \"radius.\".format(cif_id),\n                    stacklevel=2,\n                )\n                nbr_fea_idx.append(\n                    list(map(lambda x: x[2], nbr)) + [0] * (self.max_num_nbr - len(nbr))\n                )\n                nbr_fea.append(\n                    list(map(lambda x: x[1], nbr))\n                    + [self.radius + 1.0] * (self.max_num_nbr - len(nbr))\n                )\n            else:\n                nbr_fea_idx.append(list(map(lambda x: x[2], nbr[: self.max_num_nbr])))\n                nbr_fea.append(list(map(lambda x: x[1], nbr[: self.max_num_nbr])))\n        nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)\n        nbr_fea = self.gdf.expand(nbr_fea)\n        atom_fea = torch.Tensor(atom_fea)\n        nbr_fea = torch.Tensor(nbr_fea)\n        nbr_fea_idx = torch.LongTensor(nbr_fea_idx)\n        target = torch.Tensor([float(target)])\n        return (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id\n\n    def _load_item_fast(self, idx):\n        cif_id, target = self.id_prop_data[idx]\n        crystal = Structure.from_file(os.path.join(self.root_dir, cif_id + \".cif\"))\n        atom_fea = np.vstack(\n            [\n                self.ari.get_atom_fea(crystal[i].specie.number)\n                for i in range(len(crystal))\n            ]\n        )\n        atom_fea = torch.Tensor(atom_fea)\n        center_idx, neigh_idx, _images, dists = crystal.get_neighbor_list(self.radius)\n        n_sites = len(crystal)\n        bucket = [[] for _ in range(n_sites)]\n        for c, n, d in zip(center_idx, neigh_idx, dists):\n            bucket[c].append((n, d))\n        bucket = [sorted(lst, key=lambda x: x[1]) for lst in bucket]\n        nbr_fea_idx, nbr_fea = [], []\n        for lst in bucket:\n            if len(lst) &lt; self.max_num_nbr:\n                warnings.warn(\n                    f\"{cif_id} not find enough neighbors to build graph. \"\n                    \"If it happens frequently, consider increase \"\n                    \"radius.\",\n                    stacklevel=2,\n                )\n            idxs = [t[0] for t in lst[: self.max_num_nbr]]\n            dvec = [t[1] for t in lst[: self.max_num_nbr]]\n            pad = self.max_num_nbr - len(idxs)\n            nbr_fea_idx.append(idxs + [0] * pad)\n            nbr_fea.append(dvec + [self.radius + 1.0] * pad)\n        nbr_fea_idx = torch.as_tensor(np.array(nbr_fea_idx), dtype=torch.long)\n        nbr_fea = self.gdf.expand(np.array(nbr_fea))\n        nbr_fea = torch.Tensor(nbr_fea)\n        target = torch.tensor([float(target)])\n        return (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.CIFData.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the current cache.</p> Source code in <code>cgcnn2/data.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"\n    Clear the current cache.\n    \"\"\"\n    if hasattr(self._cache_load, \"cache_clear\"):\n        self._cache_load.cache_clear()\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.CIFData.set_cache_size","title":"<code>set_cache_size(cache_size)</code>","text":"<p>Change the LRU-cache capacity on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>cache_size</code> <code>int | None</code> <p>The size of the cache to set, None for unlimited size. Default is None.</p> required Source code in <code>cgcnn2/data.py</code> <pre><code>def set_cache_size(self, cache_size: Optional[int]) -&gt; None:\n    \"\"\"\n    Change the LRU-cache capacity on the fly.\n\n    Args:\n        cache_size (int | None): The size of the cache to set, None for unlimited size. Default is None.\n    \"\"\"\n    self.cache_size = cache_size\n    if hasattr(self._cache_load, \"cache_clear\"):\n        self._cache_load.cache_clear()\n    self._configure_cache()\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.CIFData_NoTarget","title":"<code>CIFData_NoTarget</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>The CIFData_NoTarget dataset is a wrapper for a dataset where the crystal structures are stored in the form of CIF files.</p> <p><code>atom_init.json</code>: a JSON file that stores the initialization vector for each element.</p> <p><code>ID.cif</code>: a CIF file that records the crystal structure, where ID is the unique ID for the crystal.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the root directory of the dataset</p> required <code>max_num_nbr</code> <code>int</code> <p>The maximum number of neighbors while constructing the crystal graph</p> <code>12</code> <code>radius</code> <code>float</code> <p>The cutoff radius for searching neighbors</p> <code>8</code> <code>dmin</code> <code>float</code> <p>The minimum distance for constructing GaussianDistance</p> <code>0</code> <code>step</code> <code>float</code> <p>The step size for constructing GaussianDistance</p> <code>0.2</code> <code>random_seed</code> <code>int</code> <p>Random seed for shuffling the dataset</p> <code>123</code> <p>Returns:</p> Name Type Description <code>atom_fea</code> <code>Tensor</code> <p>shape (n_i, atom_fea_len)</p> <code>nbr_fea</code> <code>Tensor</code> <p>shape (n_i, M, nbr_fea_len)</p> <code>nbr_fea_idx</code> <code>LongTensor</code> <p>shape (n_i, M)</p> <code>target</code> <code>Tensor</code> <p>shape (1, )</p> <code>cif_id</code> <code>str or int</code> <p>Unique ID for the crystal</p> Source code in <code>cgcnn2/data.py</code> <pre><code>class CIFData_NoTarget(Dataset):\n    \"\"\"\n    The CIFData_NoTarget dataset is a wrapper for a dataset where the crystal\n    structures are stored in the form of CIF files.\n\n    `atom_init.json`: a JSON file that stores the initialization vector for each\n    element.\n\n    `ID.cif`: a CIF file that records the crystal structure, where ID is the\n    unique ID for the crystal.\n\n    Args:\n        root_dir (str): The path to the root directory of the dataset\n        max_num_nbr (int): The maximum number of neighbors while constructing the crystal graph\n        radius (float): The cutoff radius for searching neighbors\n        dmin (float): The minimum distance for constructing GaussianDistance\n        step (float): The step size for constructing GaussianDistance\n        random_seed (int): Random seed for shuffling the dataset\n\n    Returns:\n        atom_fea (torch.Tensor): shape (n_i, atom_fea_len)\n        nbr_fea (torch.Tensor): shape (n_i, M, nbr_fea_len)\n        nbr_fea_idx (torch.LongTensor): shape (n_i, M)\n        target (torch.Tensor): shape (1, )\n        cif_id (str or int): Unique ID for the crystal\n    \"\"\"\n\n    def __init__(\n        self, root_dir, max_num_nbr=12, radius=8, dmin=0, step=0.2, random_seed=123\n    ):\n        self.root_dir = root_dir\n        self.max_num_nbr, self.radius = max_num_nbr, radius\n        assert os.path.exists(root_dir), \"root_dir does not exist!\"\n        id_prop_data = []\n        for file in os.listdir(root_dir):\n            if file.endswith(\".cif\"):\n                id_prop_data.append(file[:-4])\n        id_prop_data = [(cif_id, 0) for cif_id in id_prop_data]\n        id_prop_data.sort(key=lambda x: x[0])\n        self.id_prop_data = id_prop_data\n        random.seed(random_seed)\n        atom_init_file = os.path.join(self.root_dir, \"atom_init.json\")\n        assert os.path.exists(atom_init_file), \"atom_init.json does not exist!\"\n        self.ari = AtomCustomJSONInitializer(atom_init_file)\n        self.gdf = GaussianDistance(dmin=dmin, dmax=self.radius, step=step)\n\n    def __len__(self):\n        return len(self.id_prop_data)\n\n    @functools.lru_cache(maxsize=None)  # Cache loaded structures\n    def __getitem__(self, idx):\n        cif_id, target = self.id_prop_data[idx]\n        crystal = Structure.from_file(os.path.join(self.root_dir, cif_id + \".cif\"))\n        atom_fea = np.vstack(\n            [\n                self.ari.get_atom_fea(crystal[i].specie.number)\n                for i in range(len(crystal))\n            ]\n        )\n        atom_fea = torch.Tensor(atom_fea)\n        all_nbrs = crystal.get_all_neighbors(self.radius, include_index=True)\n        all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]\n        nbr_fea_idx, nbr_fea = [], []\n        for nbr in all_nbrs:\n            if len(nbr) &lt; self.max_num_nbr:\n                warnings.warn(\n                    \"{} not find enough neighbors to build graph. \"\n                    \"If it happens frequently, consider increase \"\n                    \"radius.\".format(cif_id),\n                    stacklevel=2,\n                )\n                nbr_fea_idx.append(\n                    list(map(lambda x: x[2], nbr)) + [0] * (self.max_num_nbr - len(nbr))\n                )\n                nbr_fea.append(\n                    list(map(lambda x: x[1], nbr))\n                    + [self.radius + 1.0] * (self.max_num_nbr - len(nbr))\n                )\n            else:\n                nbr_fea_idx.append(list(map(lambda x: x[2], nbr[: self.max_num_nbr])))\n                nbr_fea.append(list(map(lambda x: x[1], nbr[: self.max_num_nbr])))\n        nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)\n        nbr_fea = self.gdf.expand(nbr_fea)\n        atom_fea = torch.Tensor(atom_fea)\n        nbr_fea = torch.Tensor(nbr_fea)\n        nbr_fea_idx = torch.LongTensor(nbr_fea_idx)\n        target = torch.Tensor([float(target)])\n        return (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.GaussianDistance","title":"<code>GaussianDistance</code>","text":"<p>Expands the distance by Gaussian basis.</p> <p>Unit: angstrom</p> Source code in <code>cgcnn2/data.py</code> <pre><code>class GaussianDistance:\n    \"\"\"\n    Expands the distance by Gaussian basis.\n\n    Unit: angstrom\n    \"\"\"\n\n    def __init__(self, dmin, dmax, step, var=None):\n        \"\"\"\n        Args:\n            dmin (float): Minimum interatomic distance (center of the first Gaussian).\n            dmax (float): Maximum interatomic distance (center of the last Gaussian).\n            step (float): Spacing between consecutive Gaussian centers.\n            var (float, optional): Variance of each Gaussian. If None, defaults to step.\n        \"\"\"\n\n        assert dmin &lt; dmax\n        assert dmax - dmin &gt; step\n        self.filter = np.arange(dmin, dmax + step, step)\n        if var is None:\n            var = step\n        self.var = var\n\n    def expand(self, distances):\n        \"\"\"\n        Project each scalar distance onto a set of Gaussian basis functions.\n\n        Args:\n            distances (np.ndarray): An array of interatomic distances.\n\n        Returns:\n            expanded_distance (np.ndarray): An array where the last dimension contains the Gaussian basis values for each input distance.\n        \"\"\"\n\n        expanded_distance = np.exp(\n            -((distances[..., np.newaxis] - self.filter) ** 2) / self.var**2\n        )\n        return expanded_distance\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.GaussianDistance.__init__","title":"<code>__init__(dmin, dmax, step, var=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dmin</code> <code>float</code> <p>Minimum interatomic distance (center of the first Gaussian).</p> required <code>dmax</code> <code>float</code> <p>Maximum interatomic distance (center of the last Gaussian).</p> required <code>step</code> <code>float</code> <p>Spacing between consecutive Gaussian centers.</p> required <code>var</code> <code>float</code> <p>Variance of each Gaussian. If None, defaults to step.</p> <code>None</code> Source code in <code>cgcnn2/data.py</code> <pre><code>def __init__(self, dmin, dmax, step, var=None):\n    \"\"\"\n    Args:\n        dmin (float): Minimum interatomic distance (center of the first Gaussian).\n        dmax (float): Maximum interatomic distance (center of the last Gaussian).\n        step (float): Spacing between consecutive Gaussian centers.\n        var (float, optional): Variance of each Gaussian. If None, defaults to step.\n    \"\"\"\n\n    assert dmin &lt; dmax\n    assert dmax - dmin &gt; step\n    self.filter = np.arange(dmin, dmax + step, step)\n    if var is None:\n        var = step\n    self.var = var\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.GaussianDistance.expand","title":"<code>expand(distances)</code>","text":"<p>Project each scalar distance onto a set of Gaussian basis functions.</p> <p>Parameters:</p> Name Type Description Default <code>distances</code> <code>ndarray</code> <p>An array of interatomic distances.</p> required <p>Returns:</p> Name Type Description <code>expanded_distance</code> <code>ndarray</code> <p>An array where the last dimension contains the Gaussian basis values for each input distance.</p> Source code in <code>cgcnn2/data.py</code> <pre><code>def expand(self, distances):\n    \"\"\"\n    Project each scalar distance onto a set of Gaussian basis functions.\n\n    Args:\n        distances (np.ndarray): An array of interatomic distances.\n\n    Returns:\n        expanded_distance (np.ndarray): An array where the last dimension contains the Gaussian basis values for each input distance.\n    \"\"\"\n\n    expanded_distance = np.exp(\n        -((distances[..., np.newaxis] - self.filter) ** 2) / self.var**2\n    )\n    return expanded_distance\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.collate_pool","title":"<code>collate_pool(dataset_list)</code>","text":"<p>Collate a list of data and return a batch for predicting crystal properties.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_list</code> <code>list of tuples</code> <p>List of tuples for each data point. Each tuple contains:</p> required <code>atom_fea</code> <code>Tensor</code> <p>shape (n_i, atom_fea_len) Atom features for each atom in the crystal</p> required <code>nbr_fea</code> <code>Tensor</code> <p>shape (n_i, M, nbr_fea_len) Bond features for each atom's M neighbors</p> required <code>nbr_fea_idx</code> <code>LongTensor</code> <p>shape (n_i, M) Indices of M neighbors of each atom</p> required <code>target</code> <code>Tensor</code> <p>shape (1, ) Target value for prediction</p> required <p>Returns:</p> Name Type Description <code>batch_atom_fea</code> <code>Tensor</code> <p>shape (N, orig_atom_fea_len) Atom features from atom type</p> <code>batch_nbr_fea</code> <code>Tensor</code> <p>shape (N, M, nbr_fea_len) Bond features of each atom's M neighbors</p> <code>batch_nbr_fea_idx</code> <code>LongTensor</code> <p>shape (N, M) Indices of M neighbors of each atom</p> <code>crystal_atom_idx</code> <code>list of torch.LongTensor</code> <p>length N0 Mapping from the crystal idx to atom idx</p> <code>batch_target</code> <code>Tensor</code> <p>shape (N, 1) Target value for prediction</p> <code>batch_cif_ids</code> <code>list of str or int</code> <p>Unique IDs for each crystal</p> Source code in <code>cgcnn2/data.py</code> <pre><code>def collate_pool(dataset_list):\n    \"\"\"\n    Collate a list of data and return a batch for predicting crystal properties.\n\n    Args:\n        dataset_list (list of tuples): List of tuples for each data point. Each tuple contains:\n        atom_fea (torch.Tensor): shape (n_i, atom_fea_len) Atom features for each atom in the crystal\n        nbr_fea (torch.Tensor): shape (n_i, M, nbr_fea_len) Bond features for each atom's M neighbors\n        nbr_fea_idx (torch.LongTensor): shape (n_i, M) Indices of M neighbors of each atom\n        target (torch.Tensor): shape (1, ) Target value for prediction\n        cif_id (str or int) Unique ID for the crystal\n\n    Returns:\n        batch_atom_fea (torch.Tensor): shape (N, orig_atom_fea_len) Atom features from atom type\n        batch_nbr_fea (torch.Tensor): shape (N, M, nbr_fea_len) Bond features of each atom's M neighbors\n        batch_nbr_fea_idx (torch.LongTensor): shape (N, M) Indices of M neighbors of each atom\n        crystal_atom_idx (list of torch.LongTensor): length N0 Mapping from the crystal idx to atom idx\n        batch_target (torch.Tensor): shape (N, 1) Target value for prediction\n        batch_cif_ids (list of str or int): Unique IDs for each crystal\n    \"\"\"\n\n    batch_atom_fea, batch_nbr_fea, batch_nbr_fea_idx = [], [], []\n    crystal_atom_idx, batch_target = [], []\n    batch_cif_ids = []\n    base_idx = 0\n    for (atom_fea, nbr_fea, nbr_fea_idx), target, cif_id in dataset_list:\n        n_i = atom_fea.shape[0]  # number of atoms for this crystal\n        batch_atom_fea.append(atom_fea)\n        batch_nbr_fea.append(nbr_fea)\n        batch_nbr_fea_idx.append(nbr_fea_idx + base_idx)\n        new_idx = torch.LongTensor(np.arange(n_i) + base_idx)\n        crystal_atom_idx.append(new_idx)\n        batch_target.append(target)\n        batch_cif_ids.append(cif_id)\n        base_idx += n_i\n    return (\n        (\n            torch.cat(batch_atom_fea, dim=0),\n            torch.cat(batch_nbr_fea, dim=0),\n            torch.cat(batch_nbr_fea_idx, dim=0),\n            crystal_atom_idx,\n        ),\n        torch.stack(batch_target, dim=0),\n        batch_cif_ids,\n    )\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.data.full_set_split","title":"<code>full_set_split(full_set_dir, train_ratio, valid_ratio, train_force_dir=None, random_seed=0)</code>","text":"<p>Split the full set into train, valid, and test sets into a temporary directory.</p> <p>Parameters:</p> Name Type Description Default <code>full_set_dir</code> <code>str</code> <p>The path to the full set</p> required <code>train_ratio</code> <code>float</code> <p>The ratio of the training set</p> required <code>valid_ratio</code> <code>float</code> <p>The ratio of the validation set</p> required <code>train_force_dir</code> <code>str</code> <p>The path to the forced training set. Adding this will no longer keep the original split ratio.</p> <code>None</code> <code>random_seed</code> <code>int</code> <p>The random seed for the split</p> <code>0</code> <p>Returns:</p> Name Type Description <code>train_dir</code> <code>str</code> <p>The path to a temporary directory containing the train set</p> <code>valid_dir</code> <code>str</code> <p>The path to a temporary directory containing the valid set</p> <code>test_dir</code> <code>str</code> <p>The path to a temporary directory containing the test set</p> Source code in <code>cgcnn2/data.py</code> <pre><code>def full_set_split(\n    full_set_dir: str,\n    train_ratio: float,\n    valid_ratio: float,\n    train_force_dir: str | None = None,\n    random_seed: int = 0,\n):\n    \"\"\"\n    Split the full set into train, valid, and test sets into a temporary directory.\n\n    Args:\n        full_set_dir (str): The path to the full set\n        train_ratio (float): The ratio of the training set\n        valid_ratio (float): The ratio of the validation set\n        train_force_dir (str): The path to the forced training set. Adding this will no longer keep the original split ratio.\n        random_seed (int): The random seed for the split\n\n    Returns:\n        train_dir (str): The path to a temporary directory containing the train set\n        valid_dir (str): The path to a temporary directory containing the valid set\n        test_dir (str): The path to a temporary directory containing the test set\n    \"\"\"\n    df = pd.read_csv(\n        os.path.join(full_set_dir, \"id_prop.csv\"),\n        header=None,\n        names=[\"cif_id\", \"property\"],\n    )\n\n    rng = np.random.RandomState(random_seed)\n    df_shuffle = df.sample(frac=1.0, random_state=rng).reset_index(drop=True)\n\n    n_total = len(df_shuffle)\n    n_train = int(n_total * train_ratio)\n    n_valid = int(n_total * valid_ratio)\n\n    train_df = df_shuffle[:n_train]\n    valid_df = df_shuffle[n_train : n_train + n_valid]\n    test_df = df_shuffle[n_train + n_valid :]\n\n    temp_train_dir = tempfile.mkdtemp()\n    temp_valid_dir = tempfile.mkdtemp()\n    temp_test_dir = tempfile.mkdtemp()\n\n    atexit.register(shutil.rmtree, temp_train_dir, ignore_errors=True)\n    atexit.register(shutil.rmtree, temp_valid_dir, ignore_errors=True)\n    atexit.register(shutil.rmtree, temp_test_dir, ignore_errors=True)\n\n    splits = {\n        temp_train_dir: train_df,\n        temp_valid_dir: valid_df,\n        temp_test_dir: test_df,\n    }\n\n    for temp_dir, df in splits.items():\n        for cif_id in df[\"cif_id\"]:\n            src = os.path.join(full_set_dir, f\"{cif_id}.cif\")\n            dst = os.path.join(temp_dir, f\"{cif_id}.cif\")\n            shutil.copy(src, dst)\n\n    train_df.to_csv(\n        os.path.join(temp_train_dir, \"id_prop.csv\"), index=False, header=False\n    )\n    valid_df.to_csv(\n        os.path.join(temp_valid_dir, \"id_prop.csv\"), index=False, header=False\n    )\n    test_df.to_csv(\n        os.path.join(temp_test_dir, \"id_prop.csv\"), index=False, header=False\n    )\n\n    shutil.copy(os.path.join(full_set_dir, \"atom_init.json\"), temp_train_dir)\n    shutil.copy(os.path.join(full_set_dir, \"atom_init.json\"), temp_valid_dir)\n    shutil.copy(os.path.join(full_set_dir, \"atom_init.json\"), temp_test_dir)\n\n    if train_force_dir is not None:\n        df_force = pd.read_csv(\n            os.path.join(train_force_dir, \"id_prop.csv\"),\n            header=None,\n            names=[\"cif_id\", \"property\"],\n        )\n\n        train_df = pd.concat([train_df, df_force])\n        train_df.to_csv(\n            os.path.join(temp_train_dir, \"id_prop.csv\"), index=False, header=False\n        )\n\n        for cif_id in df_force[\"cif_id\"]:\n            src = os.path.join(train_force_dir, f\"{cif_id}.cif\")\n            dst = os.path.join(temp_train_dir, f\"{cif_id}.cif\")\n            shutil.copy(src, dst)\n\n    return temp_train_dir, temp_valid_dir, temp_test_dir\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.model","title":"<code>cgcnn2.model</code>","text":""},{"location":"5_api_reference/#cgcnn2.model.ConvLayer","title":"<code>ConvLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolutional layer for graph data.</p> <p>Performs a convolutional operation on graphs, updating atom features based on their neighbors.</p> Source code in <code>cgcnn2/model.py</code> <pre><code>class ConvLayer(nn.Module):\n    \"\"\"\n    Convolutional layer for graph data.\n\n    Performs a convolutional operation on graphs, updating atom features based on their neighbors.\n    \"\"\"\n\n    def __init__(self, atom_fea_len: int, nbr_fea_len: int) -&gt; None:\n        \"\"\"\n        Initialize the ConvLayer.\n\n        Args:\n            atom_fea_len (int): Number of atom hidden features.\n            nbr_fea_len (int): Number of bond (neighbor) features.\n        \"\"\"\n        super().__init__()\n        self.atom_fea_len = atom_fea_len\n        self.nbr_fea_len = nbr_fea_len\n        self.fc_full = nn.Linear(\n            2 * self.atom_fea_len + self.nbr_fea_len, 2 * self.atom_fea_len\n        )\n        self.sigmoid = nn.Sigmoid()\n        self.softplus1 = nn.Softplus()\n        self.bn1 = nn.BatchNorm1d(2 * self.atom_fea_len)\n        self.bn2 = nn.BatchNorm1d(self.atom_fea_len)\n        self.softplus2 = nn.Softplus()\n\n    def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):\n        \"\"\"\n        Forward pass.\n\n        `N`: Total number of atoms in the batch\n        `M`: Max number of neighbors\n\n        Args:\n            atom_in_fea (torch.Tensor): Tensor of shape `(N, atom_fea_len)` containing the atom features before convolution.\n            nbr_fea (torch.Tensor): Tensor of shape `(N, M, nbr_fea_len)` holding the bond features for each atom's `M` neighbors.\n            nbr_fea_idx (torch.LongTensor): Tensor of shape `(N, M)` with the indices of the `M` neighbors of each atom.\n\n        Returns:\n            atom_out_fea (torch.Tensor): Tensor of shape `(N, atom_fea_len)` with the atom features after convolution.\n\n        \"\"\"\n        N, M = nbr_fea_idx.shape\n        # Convolution\n        atom_nbr_fea = atom_in_fea[nbr_fea_idx, :]\n        total_nbr_fea = torch.cat(\n            [\n                atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),\n                atom_nbr_fea,\n                nbr_fea,\n            ],\n            dim=2,\n        )\n        total_gated_fea = self.fc_full(total_nbr_fea)\n        total_gated_fea = self.bn1(\n            total_gated_fea.view(-1, self.atom_fea_len * 2)\n        ).view(N, M, self.atom_fea_len * 2)\n        nbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2)\n        nbr_filter = self.sigmoid(nbr_filter)\n        nbr_core = self.softplus1(nbr_core)\n        nbr_sumed = torch.sum(nbr_filter * nbr_core, dim=1)\n        nbr_sumed = self.bn2(nbr_sumed)\n        atom_out_fea = self.softplus2(atom_in_fea + nbr_sumed)\n        return atom_out_fea\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.model.ConvLayer.__init__","title":"<code>__init__(atom_fea_len, nbr_fea_len)</code>","text":"<p>Initialize the ConvLayer.</p> <p>Parameters:</p> Name Type Description Default <code>atom_fea_len</code> <code>int</code> <p>Number of atom hidden features.</p> required <code>nbr_fea_len</code> <code>int</code> <p>Number of bond (neighbor) features.</p> required Source code in <code>cgcnn2/model.py</code> <pre><code>def __init__(self, atom_fea_len: int, nbr_fea_len: int) -&gt; None:\n    \"\"\"\n    Initialize the ConvLayer.\n\n    Args:\n        atom_fea_len (int): Number of atom hidden features.\n        nbr_fea_len (int): Number of bond (neighbor) features.\n    \"\"\"\n    super().__init__()\n    self.atom_fea_len = atom_fea_len\n    self.nbr_fea_len = nbr_fea_len\n    self.fc_full = nn.Linear(\n        2 * self.atom_fea_len + self.nbr_fea_len, 2 * self.atom_fea_len\n    )\n    self.sigmoid = nn.Sigmoid()\n    self.softplus1 = nn.Softplus()\n    self.bn1 = nn.BatchNorm1d(2 * self.atom_fea_len)\n    self.bn2 = nn.BatchNorm1d(self.atom_fea_len)\n    self.softplus2 = nn.Softplus()\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.model.ConvLayer.forward","title":"<code>forward(atom_in_fea, nbr_fea, nbr_fea_idx)</code>","text":"<p>Forward pass.</p> <p><code>N</code>: Total number of atoms in the batch <code>M</code>: Max number of neighbors</p> <p>Parameters:</p> Name Type Description Default <code>atom_in_fea</code> <code>Tensor</code> <p>Tensor of shape <code>(N, atom_fea_len)</code> containing the atom features before convolution.</p> required <code>nbr_fea</code> <code>Tensor</code> <p>Tensor of shape <code>(N, M, nbr_fea_len)</code> holding the bond features for each atom's <code>M</code> neighbors.</p> required <code>nbr_fea_idx</code> <code>LongTensor</code> <p>Tensor of shape <code>(N, M)</code> with the indices of the <code>M</code> neighbors of each atom.</p> required <p>Returns:</p> Name Type Description <code>atom_out_fea</code> <code>Tensor</code> <p>Tensor of shape <code>(N, atom_fea_len)</code> with the atom features after convolution.</p> Source code in <code>cgcnn2/model.py</code> <pre><code>def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):\n    \"\"\"\n    Forward pass.\n\n    `N`: Total number of atoms in the batch\n    `M`: Max number of neighbors\n\n    Args:\n        atom_in_fea (torch.Tensor): Tensor of shape `(N, atom_fea_len)` containing the atom features before convolution.\n        nbr_fea (torch.Tensor): Tensor of shape `(N, M, nbr_fea_len)` holding the bond features for each atom's `M` neighbors.\n        nbr_fea_idx (torch.LongTensor): Tensor of shape `(N, M)` with the indices of the `M` neighbors of each atom.\n\n    Returns:\n        atom_out_fea (torch.Tensor): Tensor of shape `(N, atom_fea_len)` with the atom features after convolution.\n\n    \"\"\"\n    N, M = nbr_fea_idx.shape\n    # Convolution\n    atom_nbr_fea = atom_in_fea[nbr_fea_idx, :]\n    total_nbr_fea = torch.cat(\n        [\n            atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),\n            atom_nbr_fea,\n            nbr_fea,\n        ],\n        dim=2,\n    )\n    total_gated_fea = self.fc_full(total_nbr_fea)\n    total_gated_fea = self.bn1(\n        total_gated_fea.view(-1, self.atom_fea_len * 2)\n    ).view(N, M, self.atom_fea_len * 2)\n    nbr_filter, nbr_core = total_gated_fea.chunk(2, dim=2)\n    nbr_filter = self.sigmoid(nbr_filter)\n    nbr_core = self.softplus1(nbr_core)\n    nbr_sumed = torch.sum(nbr_filter * nbr_core, dim=1)\n    nbr_sumed = self.bn2(nbr_sumed)\n    atom_out_fea = self.softplus2(atom_in_fea + nbr_sumed)\n    return atom_out_fea\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.model.CrystalGraphConvNet","title":"<code>CrystalGraphConvNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Create a crystal graph convolutional neural network for predicting total material properties.</p> Source code in <code>cgcnn2/model.py</code> <pre><code>class CrystalGraphConvNet(nn.Module):\n    \"\"\"\n    Create a crystal graph convolutional neural network for predicting total\n    material properties.\n    \"\"\"\n\n    def __init__(\n        self,\n        orig_atom_fea_len: int,\n        nbr_fea_len: int,\n        atom_fea_len: int = 64,\n        n_conv: int = 3,\n        h_fea_len: int = 128,\n        n_h: int = 1,\n        classification: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize CrystalGraphConvNet.\n\n        Args:\n            orig_atom_fea_len (int): Number of atom features in the input.\n            nbr_fea_len (int): Number of bond features.\n            atom_fea_len (int): Number of hidden atom features in the convolutional layers\n            n_conv (int): Number of convolutional layers\n            h_fea_len (int): Number of hidden features after pooling\n            n_h (int): Number of hidden layers after pooling\n            classification (bool): Whether to use classification or regression\n        \"\"\"\n        super().__init__()\n        self.classification = classification\n        self.embedding = nn.Linear(orig_atom_fea_len, atom_fea_len)\n        self.convs = nn.ModuleList(\n            [\n                ConvLayer(atom_fea_len=atom_fea_len, nbr_fea_len=nbr_fea_len)\n                for _ in range(n_conv)\n            ]\n        )\n        self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)\n        self.conv_to_fc_softplus = nn.Softplus()\n        if n_h &gt; 1:\n            self.fcs = nn.ModuleList(\n                [nn.Linear(h_fea_len, h_fea_len) for _ in range(n_h - 1)]\n            )\n            self.softpluses = nn.ModuleList([nn.Softplus() for _ in range(n_h - 1)])\n\n        if self.classification:\n            self.fc_out = nn.Linear(h_fea_len, 2)\n        else:\n            self.fc_out = nn.Linear(h_fea_len, 1)\n\n        if self.classification:\n            self.logsoftmax = nn.LogSoftmax(dim=1)\n            self.dropout = nn.Dropout()\n\n    def forward(\n        self,\n        atom_fea: torch.Tensor,\n        nbr_fea: torch.Tensor,\n        nbr_fea_idx: torch.LongTensor,\n        crystal_atom_idx: list[torch.LongTensor],\n    ):\n        \"\"\"\n        Forward pass.\n\n        `N`: Total number of atoms in the batch\n        `M`: Max number of neighbors\n        `N0`: Total number of crystals in the batch\n\n        Args:\n            atom_fea (torch.Tensor): Tensor of shape `(N, orig_atom_fea_len)` containing the atom features from atom type.\n            nbr_fea (torch.Tensor): Tensor of shape `(N, M, nbr_fea_len)` containing the bond features of each atom's `M` neighbors.\n            nbr_fea_idx (torch.LongTensor): Tensor of shape `(N, M)` containing the indices of the `M` neighbors of each atom.\n            crystal_atom_idx (list of torch.LongTensor): Mapping from the crystal index to atom index.\n\n        Returns:\n            out (torch.Tensor):\n                \u2022 `(n_crystals, 2)` if `classification=True`, containing log-probabilities.\n                \u2022 `(n_crystals, 1)` if `classification=False`, containing the regression output.\n            crys_fea (torch.Tensor): Tensor of shape `(n_crystals, h_fea_len)` containing the pooled crystal embeddings.\n\n        \"\"\"\n        atom_fea = self.embedding(atom_fea)\n        for conv_func in self.convs:\n            atom_fea = conv_func(atom_fea, nbr_fea, nbr_fea_idx)\n        crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n        crys_fea = self.conv_to_fc(self.conv_to_fc_softplus(crys_fea))\n        crys_fea = self.conv_to_fc_softplus(crys_fea)\n        if self.classification:\n            crys_fea = self.dropout(crys_fea)\n        if hasattr(self, \"fcs\") and hasattr(self, \"softpluses\"):\n            for fc, softplus in zip(self.fcs, self.softpluses):\n                crys_fea = softplus(fc(crys_fea))\n        out = self.fc_out(crys_fea)\n        if self.classification:\n            out = self.logsoftmax(out)\n        return out, crys_fea\n\n    def pooling(\n        self, atom_fea: torch.Tensor, crystal_atom_idx: list[torch.LongTensor]\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Aggregate atom features into crystal-level features by mean pooling.\n\n        Args:\n            atom_fea (torch.Tensor): Tensor of shape `(N, atom_fea_len)` containing the atom embeddings for all atoms in the batch.\n            crystal_atom_idx (list[torch.LongTensor]): List of tensors, where `crystal_atom_idx[i]` contains the indices of atoms belonging to the `i`-th crystal. The concatenated indices must cover every atom exactly once.\n\n        Returns:\n            mean_fea (torch.Tensor): Tensor of shape `(n_crystals, atom_fea_len)` containing the mean-pooled crystal embeddings.\n        \"\"\"\n        assert sum(len(idx) for idx in crystal_atom_idx) == atom_fea.shape[0]\n        mean_fea = torch.stack(\n            [torch.mean(atom_fea[idx], dim=0) for idx in crystal_atom_idx], dim=0\n        )\n        return mean_fea\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.model.CrystalGraphConvNet.__init__","title":"<code>__init__(orig_atom_fea_len, nbr_fea_len, atom_fea_len=64, n_conv=3, h_fea_len=128, n_h=1, classification=False)</code>","text":"<p>Initialize CrystalGraphConvNet.</p> <p>Parameters:</p> Name Type Description Default <code>orig_atom_fea_len</code> <code>int</code> <p>Number of atom features in the input.</p> required <code>nbr_fea_len</code> <code>int</code> <p>Number of bond features.</p> required <code>atom_fea_len</code> <code>int</code> <p>Number of hidden atom features in the convolutional layers</p> <code>64</code> <code>n_conv</code> <code>int</code> <p>Number of convolutional layers</p> <code>3</code> <code>h_fea_len</code> <code>int</code> <p>Number of hidden features after pooling</p> <code>128</code> <code>n_h</code> <code>int</code> <p>Number of hidden layers after pooling</p> <code>1</code> <code>classification</code> <code>bool</code> <p>Whether to use classification or regression</p> <code>False</code> Source code in <code>cgcnn2/model.py</code> <pre><code>def __init__(\n    self,\n    orig_atom_fea_len: int,\n    nbr_fea_len: int,\n    atom_fea_len: int = 64,\n    n_conv: int = 3,\n    h_fea_len: int = 128,\n    n_h: int = 1,\n    classification: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize CrystalGraphConvNet.\n\n    Args:\n        orig_atom_fea_len (int): Number of atom features in the input.\n        nbr_fea_len (int): Number of bond features.\n        atom_fea_len (int): Number of hidden atom features in the convolutional layers\n        n_conv (int): Number of convolutional layers\n        h_fea_len (int): Number of hidden features after pooling\n        n_h (int): Number of hidden layers after pooling\n        classification (bool): Whether to use classification or regression\n    \"\"\"\n    super().__init__()\n    self.classification = classification\n    self.embedding = nn.Linear(orig_atom_fea_len, atom_fea_len)\n    self.convs = nn.ModuleList(\n        [\n            ConvLayer(atom_fea_len=atom_fea_len, nbr_fea_len=nbr_fea_len)\n            for _ in range(n_conv)\n        ]\n    )\n    self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)\n    self.conv_to_fc_softplus = nn.Softplus()\n    if n_h &gt; 1:\n        self.fcs = nn.ModuleList(\n            [nn.Linear(h_fea_len, h_fea_len) for _ in range(n_h - 1)]\n        )\n        self.softpluses = nn.ModuleList([nn.Softplus() for _ in range(n_h - 1)])\n\n    if self.classification:\n        self.fc_out = nn.Linear(h_fea_len, 2)\n    else:\n        self.fc_out = nn.Linear(h_fea_len, 1)\n\n    if self.classification:\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n        self.dropout = nn.Dropout()\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.model.CrystalGraphConvNet.forward","title":"<code>forward(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)</code>","text":"<p>Forward pass.</p> <p><code>N</code>: Total number of atoms in the batch <code>M</code>: Max number of neighbors <code>N0</code>: Total number of crystals in the batch</p> <p>Parameters:</p> Name Type Description Default <code>atom_fea</code> <code>Tensor</code> <p>Tensor of shape <code>(N, orig_atom_fea_len)</code> containing the atom features from atom type.</p> required <code>nbr_fea</code> <code>Tensor</code> <p>Tensor of shape <code>(N, M, nbr_fea_len)</code> containing the bond features of each atom's <code>M</code> neighbors.</p> required <code>nbr_fea_idx</code> <code>LongTensor</code> <p>Tensor of shape <code>(N, M)</code> containing the indices of the <code>M</code> neighbors of each atom.</p> required <code>crystal_atom_idx</code> <code>list of torch.LongTensor</code> <p>Mapping from the crystal index to atom index.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tensor</code> <p>\u2022 <code>(n_crystals, 2)</code> if <code>classification=True</code>, containing log-probabilities. \u2022 <code>(n_crystals, 1)</code> if <code>classification=False</code>, containing the regression output.</p> <code>crys_fea</code> <code>Tensor</code> <p>Tensor of shape <code>(n_crystals, h_fea_len)</code> containing the pooled crystal embeddings.</p> Source code in <code>cgcnn2/model.py</code> <pre><code>def forward(\n    self,\n    atom_fea: torch.Tensor,\n    nbr_fea: torch.Tensor,\n    nbr_fea_idx: torch.LongTensor,\n    crystal_atom_idx: list[torch.LongTensor],\n):\n    \"\"\"\n    Forward pass.\n\n    `N`: Total number of atoms in the batch\n    `M`: Max number of neighbors\n    `N0`: Total number of crystals in the batch\n\n    Args:\n        atom_fea (torch.Tensor): Tensor of shape `(N, orig_atom_fea_len)` containing the atom features from atom type.\n        nbr_fea (torch.Tensor): Tensor of shape `(N, M, nbr_fea_len)` containing the bond features of each atom's `M` neighbors.\n        nbr_fea_idx (torch.LongTensor): Tensor of shape `(N, M)` containing the indices of the `M` neighbors of each atom.\n        crystal_atom_idx (list of torch.LongTensor): Mapping from the crystal index to atom index.\n\n    Returns:\n        out (torch.Tensor):\n            \u2022 `(n_crystals, 2)` if `classification=True`, containing log-probabilities.\n            \u2022 `(n_crystals, 1)` if `classification=False`, containing the regression output.\n        crys_fea (torch.Tensor): Tensor of shape `(n_crystals, h_fea_len)` containing the pooled crystal embeddings.\n\n    \"\"\"\n    atom_fea = self.embedding(atom_fea)\n    for conv_func in self.convs:\n        atom_fea = conv_func(atom_fea, nbr_fea, nbr_fea_idx)\n    crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n    crys_fea = self.conv_to_fc(self.conv_to_fc_softplus(crys_fea))\n    crys_fea = self.conv_to_fc_softplus(crys_fea)\n    if self.classification:\n        crys_fea = self.dropout(crys_fea)\n    if hasattr(self, \"fcs\") and hasattr(self, \"softpluses\"):\n        for fc, softplus in zip(self.fcs, self.softpluses):\n            crys_fea = softplus(fc(crys_fea))\n    out = self.fc_out(crys_fea)\n    if self.classification:\n        out = self.logsoftmax(out)\n    return out, crys_fea\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.model.CrystalGraphConvNet.pooling","title":"<code>pooling(atom_fea, crystal_atom_idx)</code>","text":"<p>Aggregate atom features into crystal-level features by mean pooling.</p> <p>Parameters:</p> Name Type Description Default <code>atom_fea</code> <code>Tensor</code> <p>Tensor of shape <code>(N, atom_fea_len)</code> containing the atom embeddings for all atoms in the batch.</p> required <code>crystal_atom_idx</code> <code>list[LongTensor]</code> <p>List of tensors, where <code>crystal_atom_idx[i]</code> contains the indices of atoms belonging to the <code>i</code>-th crystal. The concatenated indices must cover every atom exactly once.</p> required <p>Returns:</p> Name Type Description <code>mean_fea</code> <code>Tensor</code> <p>Tensor of shape <code>(n_crystals, atom_fea_len)</code> containing the mean-pooled crystal embeddings.</p> Source code in <code>cgcnn2/model.py</code> <pre><code>def pooling(\n    self, atom_fea: torch.Tensor, crystal_atom_idx: list[torch.LongTensor]\n) -&gt; torch.Tensor:\n    \"\"\"\n    Aggregate atom features into crystal-level features by mean pooling.\n\n    Args:\n        atom_fea (torch.Tensor): Tensor of shape `(N, atom_fea_len)` containing the atom embeddings for all atoms in the batch.\n        crystal_atom_idx (list[torch.LongTensor]): List of tensors, where `crystal_atom_idx[i]` contains the indices of atoms belonging to the `i`-th crystal. The concatenated indices must cover every atom exactly once.\n\n    Returns:\n        mean_fea (torch.Tensor): Tensor of shape `(n_crystals, atom_fea_len)` containing the mean-pooled crystal embeddings.\n    \"\"\"\n    assert sum(len(idx) for idx in crystal_atom_idx) == atom_fea.shape[0]\n    mean_fea = torch.stack(\n        [torch.mean(atom_fea[idx], dim=0) for idx in crystal_atom_idx], dim=0\n    )\n    return mean_fea\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils","title":"<code>cgcnn2.utils</code>","text":""},{"location":"5_api_reference/#cgcnn2.utils.Normalizer","title":"<code>Normalizer</code>","text":"<p>Normalizes a PyTorch tensor and allows restoring it later.</p> <p>This class keeps track of the mean and standard deviation of a tensor and provides methods to normalize and denormalize tensors using these statistics.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>class Normalizer:\n    \"\"\"\n    Normalizes a PyTorch tensor and allows restoring it later.\n\n    This class keeps track of the mean and standard deviation of a tensor and provides methods\n    to normalize and denormalize tensors using these statistics.\n    \"\"\"\n\n    def __init__(self, tensor: torch.Tensor) -&gt; None:\n        \"\"\"\n        Initialize the Normalizer with a sample tensor to calculate mean and standard deviation.\n\n        Args:\n            tensor (torch.Tensor): Sample tensor to compute mean and standard deviation.\n        \"\"\"\n        self.mean: torch.Tensor = torch.mean(tensor)\n        self.std: torch.Tensor = torch.std(tensor)\n\n    def norm(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Normalize a tensor using the stored mean and standard deviation.\n\n        Args:\n            tensor (torch.Tensor): Tensor to normalize.\n\n        Returns:\n            torch.Tensor: Normalized tensor.\n        \"\"\"\n        return (tensor - self.mean) / self.std\n\n    def denorm(self, normed_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Denormalize a tensor using the stored mean and standard deviation.\n\n        Args:\n            normed_tensor (torch.Tensor): Normalized tensor to denormalize.\n\n        Returns:\n            torch.Tensor: Denormalized tensor.\n        \"\"\"\n        return normed_tensor * self.std + self.mean\n\n    def state_dict(self) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Returns the state dictionary containing the mean and standard deviation.\n\n        Returns:\n            dict[str, torch.Tensor]: State dictionary.\n        \"\"\"\n        return {\"mean\": self.mean, \"std\": self.std}\n\n    def load_state_dict(self, state_dict: dict[str, torch.Tensor]) -&gt; None:\n        \"\"\"\n        Loads the mean and standard deviation from a state dictionary.\n\n        Args:\n            state_dict (dict[str, torch.Tensor]): State dictionary containing 'mean' and 'std'.\n        \"\"\"\n        self.mean = state_dict[\"mean\"]\n        self.std = state_dict[\"std\"]\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.Normalizer.__init__","title":"<code>__init__(tensor)</code>","text":"<p>Initialize the Normalizer with a sample tensor to calculate mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Sample tensor to compute mean and standard deviation.</p> required Source code in <code>cgcnn2/utils.py</code> <pre><code>def __init__(self, tensor: torch.Tensor) -&gt; None:\n    \"\"\"\n    Initialize the Normalizer with a sample tensor to calculate mean and standard deviation.\n\n    Args:\n        tensor (torch.Tensor): Sample tensor to compute mean and standard deviation.\n    \"\"\"\n    self.mean: torch.Tensor = torch.mean(tensor)\n    self.std: torch.Tensor = torch.std(tensor)\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.Normalizer.denorm","title":"<code>denorm(normed_tensor)</code>","text":"<p>Denormalize a tensor using the stored mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>normed_tensor</code> <code>Tensor</code> <p>Normalized tensor to denormalize.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Denormalized tensor.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def denorm(self, normed_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Denormalize a tensor using the stored mean and standard deviation.\n\n    Args:\n        normed_tensor (torch.Tensor): Normalized tensor to denormalize.\n\n    Returns:\n        torch.Tensor: Denormalized tensor.\n    \"\"\"\n    return normed_tensor * self.std + self.mean\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.Normalizer.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Loads the mean and standard deviation from a state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict[str, Tensor]</code> <p>State dictionary containing 'mean' and 'std'.</p> required Source code in <code>cgcnn2/utils.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, torch.Tensor]) -&gt; None:\n    \"\"\"\n    Loads the mean and standard deviation from a state dictionary.\n\n    Args:\n        state_dict (dict[str, torch.Tensor]): State dictionary containing 'mean' and 'std'.\n    \"\"\"\n    self.mean = state_dict[\"mean\"]\n    self.std = state_dict[\"std\"]\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.Normalizer.norm","title":"<code>norm(tensor)</code>","text":"<p>Normalize a tensor using the stored mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>Tensor to normalize.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Normalized tensor.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def norm(self, tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Normalize a tensor using the stored mean and standard deviation.\n\n    Args:\n        tensor (torch.Tensor): Tensor to normalize.\n\n    Returns:\n        torch.Tensor: Normalized tensor.\n    \"\"\"\n    return (tensor - self.mean) / self.std\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.Normalizer.state_dict","title":"<code>state_dict()</code>","text":"<p>Returns the state dictionary containing the mean and standard deviation.</p> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>dict[str, torch.Tensor]: State dictionary.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def state_dict(self) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Returns the state dictionary containing the mean and standard deviation.\n\n    Returns:\n        dict[str, torch.Tensor]: State dictionary.\n    \"\"\"\n    return {\"mean\": self.mean, \"std\": self.std}\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.cgcnn_descriptor","title":"<code>cgcnn_descriptor(model, loader, device, verbose)</code>","text":"<p>This function takes a pre-trained CGCNN model and a dataset, runs inference to generate predictions and features from the last layer, and returns the predictions and features. It is not necessary to have target values for the pred set.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The trained CGCNN model.</p> required <code>loader</code> <code>DataLoader</code> <p>DataLoader for the dataset.</p> required <code>device</code> <code>str</code> <p>The device ('cuda' or 'cpu') where the model will be run.</p> required <code>verbose</code> <code>int</code> <p>The verbosity level of the output.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[list[float], list[Tensor]]</code> <p>A tuple containing: - list: Model predictions - list: Crystal features from the last layer</p> Notes <p>This function is intended for use in programmatic downstream analysis, where the user wants to continue downstream analysis using predictions or features (descriptors) generated by the model. For the command-line interface, consider using the cgcnn_pr script instead.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def cgcnn_descriptor(\n    model: torch.nn.Module,\n    loader: torch.utils.data.DataLoader,\n    device: str,\n    verbose: int,\n) -&gt; tuple[list[float], list[torch.Tensor]]:\n    \"\"\"\n    This function takes a pre-trained CGCNN model and a dataset, runs inference\n    to generate predictions and features from the last layer, and returns the\n    predictions and features. It is not necessary to have target values for the\n    pred set.\n\n    Args:\n        model (torch.nn.Module): The trained CGCNN model.\n        loader (torch.utils.data.DataLoader): DataLoader for the dataset.\n        device (str): The device ('cuda' or 'cpu') where the model will be run.\n        verbose (int): The verbosity level of the output.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: Model predictions\n            - list: Crystal features from the last layer\n\n    Notes:\n        This function is intended for use in programmatic downstream analysis,\n        where the user wants to continue downstream analysis using predictions or\n        features (descriptors) generated by the model. For the command-line interface,\n        consider using the cgcnn_pr script instead.\n    \"\"\"\n\n    model.eval()\n    targets_list = []\n    outputs_list = []\n    crys_feas_list = []\n    index = 0\n\n    with torch.inference_mode():\n        for input, target, cif_id in loader:\n            atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx = input\n            atom_fea = atom_fea.to(device)\n            nbr_fea = nbr_fea.to(device)\n            nbr_fea_idx = nbr_fea_idx.to(device)\n            crystal_atom_idx = [idx_map.to(device) for idx_map in crystal_atom_idx]\n            target = target.to(device)\n\n            output, crys_fea = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n\n            targets_list.extend(target.cpu().numpy().ravel().tolist())\n            outputs_list.extend(output.cpu().numpy().ravel().tolist())\n            crys_feas_list.append(crys_fea.cpu().numpy())\n\n            index += 1\n\n            # Extract the true values from cif_id and output tensor\n            cif_id_value = cif_id[0] if cif_id and isinstance(cif_id, list) else cif_id\n            prediction_value = output.item() if output.numel() == 1 else output.tolist()\n\n            if verbose &gt;= 10:\n                logging.info(\n                    f\"index: {index} | cif id: {cif_id_value} | prediction: {prediction_value}\"\n                )\n\n    return outputs_list, crys_feas_list\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.cgcnn_pred","title":"<code>cgcnn_pred(model_path, full_set, verbose=101, cuda=False, num_workers=0)</code>","text":"<p>This function takes the path to a pre-trained CGCNN model and a dataset, runs inference to generate predictions, and returns the predictions. It is not necessary to have target values for the pred set.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the file containing the pre-trained model parameters.</p> required <code>full_set</code> <code>str</code> <p>Path to the directory containing all CIF files for the dataset.</p> required <code>verbose</code> <code>int</code> <p>Verbosity level of the output.</p> <code>101</code> <code>cuda</code> <code>bool</code> <p>Whether to use CUDA.</p> <code>False</code> <code>num_workers</code> <code>int</code> <p>Number of subprocesses for data loading.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[list[float], list[Tensor]]</code> <p>A tuple containing: - list: Model predictions - list: Features from the last layer</p> Notes <p>This function is intended for use in programmatic downstream analysis, where the user wants to continue downstream analysis using predictions or features (descriptors) generated by the model. For the command-line interface, consider using the cgcnn_pr script instead.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def cgcnn_pred(\n    model_path: str,\n    full_set: str,\n    verbose: int = 101,\n    cuda: bool = False,\n    num_workers: int = 0,\n) -&gt; tuple[list[float], list[torch.Tensor]]:\n    \"\"\"\n    This function takes the path to a pre-trained CGCNN model and a dataset,\n    runs inference to generate predictions, and returns the predictions. It is\n    not necessary to have target values for the pred set.\n\n    Args:\n        model_path (str): Path to the file containing the pre-trained model parameters.\n        full_set (str): Path to the directory containing all CIF files for the dataset.\n        verbose (int): Verbosity level of the output.\n        cuda (bool): Whether to use CUDA.\n        num_workers (int): Number of subprocesses for data loading.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: Model predictions\n            - list: Features from the last layer\n\n    Notes:\n        This function is intended for use in programmatic downstream analysis,\n        where the user wants to continue downstream analysis using predictions or\n        features (descriptors) generated by the model. For the command-line interface,\n        consider using the cgcnn_pr script instead.\n    \"\"\"\n    if not os.path.isfile(model_path):\n        raise FileNotFoundError(f\"=&gt; No model params found at '{model_path}'\")\n\n    total_dataset = CIFData_NoTarget(full_set)\n\n    checkpoint = torch.load(\n        model_path,\n        map_location=lambda storage, loc: storage if not cuda else None,\n        weights_only=False,\n    )\n    structures, _, _ = total_dataset[0]\n    orig_atom_fea_len = structures[0].shape[-1]\n    nbr_fea_len = structures[1].shape[-1]\n    model_args = argparse.Namespace(**checkpoint[\"args\"])\n    model = CrystalGraphConvNet(\n        orig_atom_fea_len,\n        nbr_fea_len,\n        atom_fea_len=model_args.atom_fea_len,\n        n_conv=model_args.n_conv,\n        h_fea_len=model_args.h_fea_len,\n        n_h=model_args.n_h,\n    )\n    if cuda:\n        model.cuda()\n\n    normalizer = Normalizer(torch.zeros(3))\n    normalizer.load_state_dict(checkpoint[\"normalizer\"])\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\n    if verbose &gt;= 100:\n        print_checkpoint_info(checkpoint, model_path)\n\n    device = \"cuda\" if cuda else \"cpu\"\n    model.to(device).eval()\n\n    full_loader = DataLoader(\n        total_dataset,\n        batch_size=1,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=collate_pool,\n        pin_memory=cuda,\n    )\n\n    pred, last_layer = cgcnn_descriptor(model, full_loader, device, verbose)\n\n    return pred, last_layer\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.cgcnn_test","title":"<code>cgcnn_test(model, loader, device, results_file='results.csv', plot_file='parity_plot.png', axis_limits=None, **kwargs)</code>","text":"<p>This function takes a pre-trained CGCNN model and a test dataset, runs inference to generate predictions, creates a parity plot comparing pred versus true values, and writes the results to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The pre-trained CGCNN model.</p> required <code>loader</code> <code>DataLoader</code> <p>DataLoader for the dataset.</p> required <code>device</code> <code>str</code> <p>The device ('cuda' or 'cpu') where the model will be run.</p> required <code>results_file</code> <code>str</code> <p>File path for saving results as CSV.</p> <code>'results.csv'</code> <code>plot_file</code> <code>str</code> <p>File path for saving the parity plot.</p> <code>'parity_plot.png'</code> <code>axis_limits</code> <code>list</code> <p>Limits for x-axis (true values) of the parity plot.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments: xlabel (str): x-axis label for the parity plot. ylabel (str): y-axis label for the parity plot.</p> <code>{}</code> Notes <p>This function is intended for use in a command-line interface, providing direct output of results. For programmatic downstream analysis, consider using the API functions instead, i.e. cgcnn_pred and cgcnn_descriptor.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def cgcnn_test(\n    model: torch.nn.Module,\n    loader: torch.utils.data.DataLoader,\n    device: str,\n    results_file: str = \"results.csv\",\n    plot_file: str = \"parity_plot.png\",\n    axis_limits: list[float] | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    This function takes a pre-trained CGCNN model and a test dataset, runs\n    inference to generate predictions, creates a parity plot comparing pred\n    versus true values, and writes the results to a CSV file.\n\n    Args:\n        model (torch.nn.Module): The pre-trained CGCNN model.\n        loader (torch.utils.data.DataLoader): DataLoader for the dataset.\n        device (str): The device ('cuda' or 'cpu') where the model will be run.\n        results_file (str, optional): File path for saving results as CSV.\n        plot_file (str, optional): File path for saving the parity plot.\n        axis_limits (list, optional): Limits for x-axis (true values) of the parity plot.\n        **kwargs: Additional keyword arguments:\n            xlabel (str): x-axis label for the parity plot.\n            ylabel (str): y-axis label for the parity plot.\n\n    Notes:\n        This function is intended for use in a command-line interface, providing\n        direct output of results. For programmatic downstream analysis, consider\n        using the API functions instead, i.e. cgcnn_pred and cgcnn_descriptor.\n    \"\"\"\n\n    # Extract optional plot labels from kwargs\n    xlabel = kwargs.get(\"xlabel\", \"true\")\n    ylabel = kwargs.get(\"ylabel\", \"pred\")\n\n    model.eval()\n    targets_list = []\n    outputs_list = []\n    cif_ids = []\n\n    with torch.inference_mode():\n        for input_batch, target, cif_id in loader:\n            atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx = input_batch\n            atom_fea = atom_fea.to(device)\n            nbr_fea = nbr_fea.to(device)\n            nbr_fea_idx = nbr_fea_idx.to(device)\n            crystal_atom_idx = [idx_map.to(device) for idx_map in crystal_atom_idx]\n            target = target.to(device)\n            output, _ = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n\n            targets_list.extend(target.cpu().numpy().ravel().tolist())\n            outputs_list.extend(output.cpu().numpy().ravel().tolist())\n            cif_ids.extend(cif_id)\n\n    targets_array = np.array(targets_list)\n    outputs_array = np.array(outputs_list)\n\n    # MSE and R2 Score\n    mse = np.mean((targets_array - outputs_array) ** 2)\n    ss_res = np.sum((targets_array - outputs_array) ** 2)\n    ss_tot = np.sum((targets_array - np.mean(targets_array)) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    logging.info(f\"MSE: {mse:.4f}, R2 Score: {r2:.4f}\")\n\n    # Save results to CSV\n    sorted_rows = sorted(zip(cif_ids, targets_list, outputs_list), key=lambda x: x[0])\n    with open(results_file, \"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerow([\"cif_id\", \"true\", \"pred\"])\n        writer.writerows(sorted_rows)\n    logging.info(f\"Prediction results have been saved to {results_file}\")\n\n    # Create parity plot\n    df_full = pd.DataFrame({\"true\": targets_list, \"pred\": outputs_list})\n    plot_hexbin(df_full, xlabel, ylabel, out_png=plot_file)\n    logging.info(f\"Hexbin plot has been saved to {plot_file}\")\n\n    # If axis limits are provided, save the csv file with the specified limits\n    if axis_limits:\n        df_clip = df_full[\n            (df_full[\"true\"] &gt;= axis_limits[0]) &amp; (df_full[\"true\"] &lt;= axis_limits[1])\n        ]\n        clipped_file = plot_file.replace(\n            \".png\", f\"_axis_limits_{axis_limits[0]}_{axis_limits[1]}.png\"\n        )\n        plot_hexbin(df_clip, xlabel, ylabel, out_png=clipped_file)\n        logging.info(\n            f\"Hexbin plot clipped to {axis_limits} on true has been saved to {clipped_file}\"\n        )\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.get_local_version","title":"<code>get_local_version()</code>","text":"<p>Retrieves the version of the project from the pyproject.toml file.</p> <p>Returns:</p> Name Type Description <code>version</code> <code>str</code> <p>The version of the project.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def get_local_version() -&gt; str:\n    \"\"\"\n    Retrieves the version of the project from the pyproject.toml file.\n\n    Returns:\n        version (str): The version of the project.\n    \"\"\"\n    project_root = Path(__file__).parents[2]\n    toml_path = project_root / \"pyproject.toml\"\n    try:\n        with toml_path.open(\"rb\") as f:\n            data = tomllib.load(f)\n            version = data[\"project\"][\"version\"]\n        return version\n    except Exception:\n        return \"unknown\"\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.get_lr","title":"<code>get_lr(optimizer)</code>","text":"<p>Extracts learning rates from a PyTorch optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The PyTorch optimizer to extract learning rates from.</p> required <p>Returns:</p> Name Type Description <code>learning_rates</code> <code>list[float]</code> <p>A list of learning rates for each parameter group.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def get_lr(optimizer: torch.optim.Optimizer) -&gt; list[float]:\n    \"\"\"\n    Extracts learning rates from a PyTorch optimizer.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The PyTorch optimizer to extract learning rates from.\n\n    Returns:\n        learning_rates (list[float]): A list of learning rates for each parameter group.\n    \"\"\"\n\n    learning_rates = []\n    for param_group in optimizer.param_groups:\n        learning_rates.append(param_group[\"lr\"])\n    return learning_rates\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.id_prop_gen","title":"<code>id_prop_gen(cif_dir)</code>","text":"<p>Generates a CSV file containing IDs and properties of CIF files.</p> <p>Parameters:</p> Name Type Description Default <code>cif_dir</code> <code>str</code> <p>Directory containing the CIF files.</p> required Source code in <code>cgcnn2/utils.py</code> <pre><code>def id_prop_gen(cif_dir: str) -&gt; None:\n    \"\"\"Generates a CSV file containing IDs and properties of CIF files.\n\n    Args:\n        cif_dir (str): Directory containing the CIF files.\n    \"\"\"\n\n    cif_list = glob.glob(f\"{cif_dir}/*.cif\")\n\n    id_prop_cif = pd.DataFrame(\n        {\n            \"id\": [os.path.basename(cif).split(\".\")[0] for cif in cif_list],\n            \"prop\": [0 for _ in range(len(cif_list))],\n        }\n    )\n\n    id_prop_cif.to_csv(\n        f\"{cif_dir}/id_prop.csv\",\n        index=False,\n        header=False,\n    )\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.metrics_text","title":"<code>metrics_text(df, metrics=['mae', 'r2'], metrics_precision='3f', unit=None, unit_scale=1.0)</code>","text":"<p>Create a text string containing the metrics and their values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the true and pred values.</p> required <code>metrics</code> <code>list[str]</code> <p>A list of metrics to be displayed in the plot.</p> <code>['mae', 'r2']</code> <code>metrics_precision</code> <code>str</code> <p>Format string for the metrics.</p> <code>'3f'</code> <code>unit</code> <code>str | None</code> <p>Unit of the property.</p> <code>None</code> <code>unit_scale</code> <code>float</code> <p>Scale factor for the unit.</p> <code>1.0</code> <p>Returns:     text (str): A text string containing the metrics and their values.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def metrics_text(\n    df: pd.DataFrame,\n    metrics: list[str] = [\"mae\", \"r2\"],\n    metrics_precision: str = \"3f\",\n    unit: str | None = None,\n    unit_scale: float = 1.0,\n) -&gt; str:\n    \"\"\"\n    Create a text string containing the metrics and their values.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the true and pred values.\n        metrics (list[str]): A list of metrics to be displayed in the plot.\n        metrics_precision (str): Format string for the metrics.\n        unit (str | None): Unit of the property.\n        unit_scale (float): Scale factor for the unit.\n    Returns:\n        text (str): A text string containing the metrics and their values.\n    \"\"\"\n\n    values: dict[str, float] = {}\n    for m in metrics:\n        m_lower = m.lower()\n        if m_lower == \"mae\":\n            values[\"MAE\"] = np.mean(np.abs(df[\"true\"] - df[\"pred\"])) * unit_scale\n        elif m_lower == \"mse\":\n            values[\"MSE\"] = np.mean((df[\"true\"] - df[\"pred\"]) ** 2) * unit_scale\n        elif m_lower == \"rmse\":\n            values[\"RMSE\"] = (\n                np.sqrt(np.mean((df[\"true\"] - df[\"pred\"]) ** 2)) * unit_scale\n            )\n        elif m_lower == \"r2\":\n            values[\"R^2\"] = 1 - np.sum((df[\"true\"] - df[\"pred\"]) ** 2) / np.sum(\n                (df[\"true\"] - df[\"true\"].mean()) ** 2\n            )\n        else:\n            raise ValueError(f\"Unsupported metric: {m}\")\n\n    text_lines: list[str] = []\n    for name, val in values.items():\n        if unit and name == \"MSE\":\n            unit_str = rf\"\\,\\mathrm{{{unit}}}^2\"\n        elif unit and name != \"R^2\":\n            unit_str = rf\"\\,\\mathrm{{{unit}}}\"\n        else:\n            unit_str = \"\"\n\n        if name == \"R^2\":\n            latex_name = r\"R^2\"\n        else:\n            latex_name = rf\"\\mathrm{{{name}}}\"\n\n        if name == \"R^2\":\n            text_lines.append(rf\"${latex_name}: {val:.3f}{unit_str}$\")\n        else:\n            text_lines.append(rf\"${latex_name}: {val:.{metrics_precision}}{unit_str}$\")\n    text = \"\\n\".join(text_lines)\n\n    return text\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.output_id_gen","title":"<code>output_id_gen()</code>","text":"<p>Generates a unique output identifier based on current date and time.</p> <p>Returns:</p> Name Type Description <code>folder_name</code> <code>str</code> <p>A string in format 'output_mmdd_HHMM' for current date/time.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def output_id_gen() -&gt; str:\n    \"\"\"\n    Generates a unique output identifier based on current date and time.\n\n    Returns:\n        folder_name (str): A string in format 'output_mmdd_HHMM' for current date/time.\n    \"\"\"\n\n    now = datetime.now()\n    timestamp = now.strftime(\"%m%d_%H%M\")\n    folder_name = f\"output_{timestamp}\"\n\n    return folder_name\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.plot_convergence","title":"<code>plot_convergence(df, xlabel, ylabel, ax=None, y2label=None, ylabel_precision='3f', y2label_precision='3f', colors=('#137DC5', '#BF1922'), xtick_rotation=0, subfigure_label=None, out_png=None)</code>","text":"<p>Create a convergence plot and save it to a file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the metrics values.</p> required <code>xlabel</code> <code>str</code> <p>Label for the x-axis (epochs)</p> required <code>ylabel</code> <code>str</code> <p>Label for the y-axis (metric)</p> required <code>ax</code> <code>Axes | None</code> <p>Axes object to plot the convergence on.</p> <code>None</code> <code>y2label</code> <code>str | None</code> <p>Label for the y2-axis (metric)</p> <code>None</code> <code>ylabel_precision</code> <code>str</code> <p>Format string for the y-axis label.</p> <code>'3f'</code> <code>y2label_precision</code> <code>str</code> <p>Format string for the y2-axis label.</p> <code>'3f'</code> <code>colors</code> <code>Sequence[str]</code> <p>Colors for the lines.</p> <code>('#137DC5', '#BF1922')</code> <code>xtick_rotation</code> <code>float</code> <p>Rotation of the x-axis tick labels.</p> <code>0</code> <code>subfigure_label</code> <code>str | None</code> <p>Label for the subfigure.</p> <code>None</code> <code>out_png</code> <code>str | None</code> <p>Path of the PNG file in which to save the convergence plot.</p> <code>None</code> Source code in <code>cgcnn2/utils.py</code> <pre><code>def plot_convergence(\n    df: pd.DataFrame,\n    xlabel: str,\n    ylabel: str,\n    ax: plt.Axes | None = None,\n    y2label: str | None = None,\n    ylabel_precision: str = \"3f\",\n    y2label_precision: str = \"3f\",\n    colors: Sequence[str] = (\"#137DC5\", \"#BF1922\"),\n    xtick_rotation: float = 0,\n    subfigure_label: str | None = None,\n    out_png: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Create a convergence plot and save it to a file.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the metrics values.\n        xlabel (str): Label for the x-axis (epochs)\n        ylabel (str): Label for the y-axis (metric)\n        ax (plt.Axes | None): Axes object to plot the convergence on.\n        y2label (str | None): Label for the y2-axis (metric)\n        ylabel_precision (str): Format string for the y-axis label.\n        y2label_precision (str): Format string for the y2-axis label.\n        colors (Sequence[str]): Colors for the lines.\n        xtick_rotation (float): Rotation of the x-axis tick labels.\n        subfigure_label (str | None): Label for the subfigure.\n        out_png (str | None): Path of the PNG file in which to save the convergence plot.\n\n    \"\"\"\n\n    with plt.rc_context(PLOT_RC_PARAMS):\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(8, 6), layout=\"constrained\")\n        else:\n            fig = ax.get_figure()\n\n        x = df[xlabel]\n        y = df[ylabel]\n\n        # Primary line (left y\u2011axis)\n        (ln1,) = ax.plot(x, y, label=ylabel, color=colors[0])\n\n        lines = [ln1]\n        labels = [ylabel]\n\n        # Optional secondary line (right y\u2011axis)\n        if y2label is not None:\n            y2 = df[y2label]\n            ax2 = ax.twinx()\n\n            (ln2,) = ax2.plot(x, y2, linestyle=\"--\", label=y2label, color=colors[1])\n\n            lines.append(ln2)\n            labels.append(y2label)\n\n            y1_lim = ax.get_ylim()\n            y2_lim = ax2.get_ylim()\n\n            ax.set_yticks(np.linspace(y1_lim[0], y1_lim[1], 6))\n            ax2.set_yticks(np.linspace(y2_lim[0], y2_lim[1], 6))\n\n            ax.yaxis.set_major_formatter(\n                mticker.FormatStrFormatter(f\"%.{ylabel_precision}\")\n            )\n            ax2.yaxis.set_major_formatter(\n                mticker.FormatStrFormatter(f\"%.{y2label_precision}\")\n            )\n\n            ax.yaxis.set_minor_locator(mticker.AutoMinorLocator(2))\n            ax2.yaxis.set_minor_locator(mticker.AutoMinorLocator(2))\n\n            ax.legend(lines, labels, loc=\"center right\")\n\n            ax.spines[\"left\"].set_color(colors[0])\n            ax2.spines[\"left\"].set_visible(False)\n            ax2.spines[\"right\"].set_color(colors[1])\n            ax.spines[\"right\"].set_visible(False)\n\n            ax.tick_params(axis=\"y\", colors=colors[0], which=\"both\")\n            ax2.tick_params(axis=\"y\", colors=colors[1], which=\"both\")\n\n        else:\n            ax.set_xlabel(xlabel)\n            ax.set_ylabel(ylabel)\n\n        ax.tick_params(axis=\"x\", rotation=xtick_rotation)\n\n        ax.grid(True, which=\"major\", alpha=0.3)\n\n        if subfigure_label is not None:\n            ax.text(\n                0.025,\n                0.975,\n                subfigure_label,\n                transform=ax.transAxes,\n                ha=\"left\",\n                va=\"top\",\n            )\n\n        if out_png is not None:\n            fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.plot_hexbin","title":"<code>plot_hexbin(df, xlabel, ylabel, ax=None, metrics=['mae', 'r2'], metrics_precision='3f', unit=None, unit_scale=1.0, subfigure_label=None, out_png=None)</code>","text":"<p>Create a hexbin plot and save it to a file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the true and pred values.</p> required <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> required <code>ylabel</code> <code>str</code> <p>Label for the y-axis.</p> required <code>ax</code> <code>Axes | None</code> <p>Axes object to plot the hexbin on.</p> <code>None</code> <code>metrics</code> <code>list[str]</code> <p>A list of strings to be displayed in the plot.</p> <code>['mae', 'r2']</code> <code>metrics_precision</code> <code>str</code> <p>Format string for the metrics.</p> <code>'3f'</code> <code>unit</code> <code>str | None</code> <p>Unit of the property.</p> <code>None</code> <code>unit_scale</code> <code>float</code> <p>Scale factor for the unit.</p> <code>1.0</code> <code>subfigure_label</code> <code>str | None</code> <p>Label for the subfigure.</p> <code>None</code> <code>out_png</code> <code>str | None</code> <p>Path of the PNG file in which to save the hexbin plot.</p> <code>None</code> Source code in <code>cgcnn2/utils.py</code> <pre><code>def plot_hexbin(\n    df: pd.DataFrame,\n    xlabel: str,\n    ylabel: str,\n    ax: plt.Axes | None = None,\n    metrics: list[str] = [\"mae\", \"r2\"],\n    metrics_precision: str = \"3f\",\n    unit: str | None = None,\n    unit_scale: float = 1.0,\n    subfigure_label: str | None = None,\n    out_png: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Create a hexbin plot and save it to a file.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the true and pred values.\n        xlabel (str): Label for the x-axis.\n        ylabel (str): Label for the y-axis.\n        ax (plt.Axes | None): Axes object to plot the hexbin on.\n        metrics (list[str]): A list of strings to be displayed in the plot.\n        metrics_precision (str): Format string for the metrics.\n        unit (str | None): Unit of the property.\n        unit_scale (float): Scale factor for the unit.\n        subfigure_label (str | None): Label for the subfigure.\n        out_png (str | None): Path of the PNG file in which to save the hexbin plot.\n\n    \"\"\"\n\n    with plt.rc_context(PLOT_RC_PARAMS):\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(8, 6), layout=\"constrained\")\n        else:\n            ax.get_figure()\n\n        hb = ax.hexbin(\n            x=\"true\",\n            y=\"pred\",\n            data=df,\n            gridsize=40,\n            cmap=\"viridis\",\n            bins=\"log\",\n        )\n\n        ax.set_xlabel(xlabel)\n        ax.set_ylabel(ylabel)\n\n        # Keep axes square\n        ax.set_box_aspect(1)\n\n        # Get the current axis limits\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        min_val = min(xlim[0], ylim[0])\n        max_val = max(xlim[1], ylim[1])\n\n        # Plot y = x reference line (grey dashed)\n        ax.plot(\n            [min_val, max_val],\n            [min_val, max_val],\n            linestyle=\"--\",\n            color=\"grey\",\n            linewidth=2,\n        )\n\n        # Restore the original limits\n        ax.set_xlim(xlim)\n        ax.set_ylim(ylim)\n\n        # add density colorbar put inside the plot\n        cax = inset_axes(\n            ax, width=\"3.5%\", height=\"70%\", loc=\"lower right\", borderpad=0.5\n        )\n        plt.colorbar(hb, cax=cax)\n        cax.yaxis.set_ticks_position(\"left\")\n        cax.yaxis.set_label_position(\"left\")\n\n        # Compute requested metrics\n        text = metrics_text(df, metrics, metrics_precision, unit, unit_scale)\n\n        if subfigure_label is not None:\n            text = f\"{subfigure_label}\\n{text}\"\n\n        ax.text(\n            0.025,\n            0.975,\n            text,\n            transform=ax.transAxes,\n            ha=\"left\",\n            va=\"top\",\n        )\n\n        if out_png is not None:\n            plt.savefig(out_png, format=\"png\", dpi=300, bbox_inches=\"tight\")\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.plot_scatter","title":"<code>plot_scatter(df, xlabel, ylabel, ax=None, true_types=['true_train', 'true_valid', 'true_test'], pred_types=['pred_train', 'pred_valid', 'pred_test'], colors=('#137DC5', '#FACF39', '#BF1922', '#F7E8D3', '#B89FDC', '#0F0C08'), legend_labels=None, metrics=['mae', 'r2'], metrics_precision='3f', unit=None, unit_scale=1.0, subfigure_label=None, out_png=None)</code>","text":"<p>Create a scatter plot and save it to a file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the true and pred values.</p> required <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> required <code>ylabel</code> <code>str</code> <p>Label for the y-axis.</p> required <code>ax</code> <code>Axes | None</code> <p>Axes object to plot the scatter on.</p> <code>None</code> <code>true_types</code> <code>list[str]</code> <p>A list of true data types to be displayed in the plot.</p> <code>['true_train', 'true_valid', 'true_test']</code> <code>pred_types</code> <code>list[str]</code> <p>A list of pred data types to be displayed in the plot.</p> <code>['pred_train', 'pred_valid', 'pred_test']</code> <code>colors</code> <code>Sequence[str]</code> <p>A list of colors to be used for the data types. Default palette is adapted from Looka 2025 with six colors.</p> <code>('#137DC5', '#FACF39', '#BF1922', '#F7E8D3', '#B89FDC', '#0F0C08')</code> <code>legend_labels</code> <code>list[str] | None</code> <p>A list of labels for the legend.</p> <code>None</code> <code>metrics</code> <code>list[str]</code> <p>Metrics to display in the plot.</p> <code>['mae', 'r2']</code> <code>metrics_precision</code> <code>str</code> <p>Format string for the metrics.</p> <code>'3f'</code> <code>unit</code> <code>str | None</code> <p>Unit of the property.</p> <code>None</code> <code>unit_scale</code> <code>float</code> <p>Scale factor for the unit.</p> <code>1.0</code> <code>subfigure_label</code> <code>str | None</code> <p>Label for the subfigure.</p> <code>None</code> <code>out_png</code> <code>str | None</code> <p>Path of the PNG file in which to save the scatter plot.</p> <code>None</code> Source code in <code>cgcnn2/utils.py</code> <pre><code>def plot_scatter(\n    df: pd.DataFrame,\n    xlabel: str,\n    ylabel: str,\n    ax: plt.Axes | None = None,\n    true_types: list[str] = [\"true_train\", \"true_valid\", \"true_test\"],\n    pred_types: list[str] = [\"pred_train\", \"pred_valid\", \"pred_test\"],\n    colors: Sequence[str] = (\n        \"#137DC5\",\n        \"#FACF39\",\n        \"#BF1922\",\n        \"#F7E8D3\",\n        \"#B89FDC\",\n        \"#0F0C08\",\n    ),\n    legend_labels: list[str] | None = None,\n    metrics: list[str] = [\"mae\", \"r2\"],\n    metrics_precision: str = \"3f\",\n    unit: str | None = None,\n    unit_scale: float = 1.0,\n    subfigure_label: str | None = None,\n    out_png: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Create a scatter plot and save it to a file.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the true and pred values.\n        xlabel (str): Label for the x-axis.\n        ylabel (str): Label for the y-axis.\n        ax (plt.Axes | None): Axes object to plot the scatter on.\n        true_types (list[str]): A list of true data types to be displayed in the plot.\n        pred_types (list[str]): A list of pred data types to be displayed in the plot.\n        colors (Sequence[str]): A list of colors to be used for the data types.\n            Default palette is adapted from\n            [Looka 2025](https://looka.com/blog/logo-color-trends/) with six colors.\n        legend_labels (list[str] | None): A list of labels for the legend.\n        metrics (list[str]): Metrics to display in the plot.\n        metrics_precision (str): Format string for the metrics.\n        unit (str | None): Unit of the property.\n        unit_scale (float): Scale factor for the unit.\n        subfigure_label (str | None): Label for the subfigure.\n        out_png (str | None): Path of the PNG file in which to save the scatter plot.\n\n    \"\"\"\n\n    with plt.rc_context(PLOT_RC_PARAMS):\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(8, 6), layout=\"constrained\")\n        else:\n            ax.get_figure()\n\n        for true_type, pred_type in zip(true_types, pred_types):\n            ax.scatter(\n                x=true_type,\n                y=pred_type,\n                data=df,\n                c=colors[true_types.index(true_type) % len(colors)],\n                alpha=0.5,\n            )\n\n        ax.set_xlabel(xlabel)\n        ax.set_ylabel(ylabel)\n\n        # Keep axes square\n        ax.set_box_aspect(1)\n\n        # Get the current axis limits\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        axis_min = min(xlim[0], ylim[0])\n        axis_max = max(xlim[1], ylim[1])\n\n        # Plot y = x reference line (grey dashed)\n        ax.plot(\n            [axis_min, axis_max],\n            [axis_min, axis_max],\n            linestyle=\"--\",\n            color=\"grey\",\n            linewidth=2,\n        )\n\n        # Restore the original limits\n        ax.set_xlim(xlim)\n        ax.set_ylim(ylim)\n\n        # Convert test data for metrics calculation\n        df_metrics = df.rename(\n            columns={\n                \"true_test\": \"true\",\n                \"pred_test\": \"pred\",\n            }\n        )\n\n        # Compute requested metrics\n        text = metrics_text(df_metrics, metrics, metrics_precision, unit, unit_scale)\n\n        if subfigure_label is not None:\n            text = f\"{subfigure_label}\\n{text}\"\n\n        ax.text(\n            0.025,\n            0.975,\n            text,\n            transform=ax.transAxes,\n            ha=\"left\",\n            va=\"top\",\n        )\n\n        if legend_labels is not None:\n            if len(legend_labels) != len(true_types):\n                raise ValueError(\n                    f\"legend_labels length ({len(legend_labels)}) must match number of data series ({len(true_types)})\"\n                )\n            ax.legend(legend_labels, loc=\"lower right\")\n\n        if out_png is not None:\n            plt.savefig(out_png, format=\"png\", dpi=300, bbox_inches=\"tight\")\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.print_checkpoint_info","title":"<code>print_checkpoint_info(checkpoint, model_path)</code>","text":"<p>Prints the checkpoint information.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>dict[str, Any]</code> <p>The checkpoint dictionary.</p> required <code>model_path</code> <code>str</code> <p>The path to the model file.</p> required Source code in <code>cgcnn2/utils.py</code> <pre><code>def print_checkpoint_info(checkpoint: dict[str, Any], model_path: str) -&gt; None:\n    \"\"\"\n    Prints the checkpoint information.\n\n    Args:\n        checkpoint (dict[str, Any]): The checkpoint dictionary.\n        model_path (str): The path to the model file.\n    \"\"\"\n    epoch = checkpoint.get(\"epoch\", \"N/A\")\n    mse = checkpoint.get(\"best_mse_error\")\n    mae = checkpoint.get(\"best_mae_error\")\n\n    metrics = []\n    if mse is not None:\n        metrics.append(f\"MSE={mse:.4f}\")\n    if mae is not None:\n        metrics.append(f\"MAE={mae:.4f}\")\n\n    metrics_str = \", \".join(metrics) if metrics else \"N/A\"\n\n    logging.info(\n        f\"=&gt; Loaded model from '{model_path}' (epoch {epoch}, validation {metrics_str})\"\n    )\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.seed_everything","title":"<code>seed_everything(seed)</code>","text":"<p>Seeds the random number generators for Python, NumPy, PyTorch, and PyTorch CUDA.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The seed value to use for random number generation.</p> required Source code in <code>cgcnn2/utils.py</code> <pre><code>def seed_everything(seed: int) -&gt; None:\n    \"\"\"\n    Seeds the random number generators for Python, NumPy, PyTorch, and PyTorch CUDA.\n\n    Args:\n        seed (int): The seed value to use for random number generation.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.setup_logging","title":"<code>setup_logging()</code>","text":"<p>Sets up logging for the project.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def setup_logging() -&gt; None:\n    \"\"\"\n    Sets up logging for the project.\n    \"\"\"\n    logging.basicConfig(\n        stream=sys.stdout,\n        level=logging.INFO,\n        format=\"%(asctime)s.%(msecs)03d %(levelname)s: %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    logging.captureWarnings(True)\n\n    torch_ver: str = getattr(torch, \"__version__\", \"unknown\")\n    cuda_ver: Optional[str] = _torch_cuda_version\n\n    logging.info(f\"cgcnn2 version: {cgcnn2.__version__}\")\n    logging.info(f\"cuda version: {cuda_ver}\")\n    logging.info(f\"torch version: {torch_ver}\")\n</code></pre>"},{"location":"5_api_reference/#cgcnn2.utils.unique_structures_clean","title":"<code>unique_structures_clean(dataset_dir, delete_duplicates=False)</code>","text":"<p>Checks for duplicate (structurally equivalent) structures in a directory of CIF files using pymatgen's StructureMatcher and returns the count of unique structures.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>str</code> <p>The path to the dataset containing CIF files.</p> required <code>delete_duplicates</code> <code>bool</code> <p>Whether to delete the duplicate structures.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>grouped</code> <code>list</code> <p>A list of lists, where each sublist contains structurally equivalent structures.</p> Source code in <code>cgcnn2/utils.py</code> <pre><code>def unique_structures_clean(dataset_dir, delete_duplicates=False):\n    \"\"\"\n    Checks for duplicate (structurally equivalent) structures in a directory\n    of CIF files using pymatgen's StructureMatcher and returns the count\n    of unique structures.\n\n    Args:\n        dataset_dir (str): The path to the dataset containing CIF files.\n        delete_duplicates (bool): Whether to delete the duplicate structures.\n\n    Returns:\n        grouped (list): A list of lists, where each sublist contains structurally equivalent structures.\n    \"\"\"\n    cif_files = [f for f in os.listdir(dataset_dir) if f.endswith(\".cif\")]\n    structures = []\n    filenames = []\n\n    for fname in cif_files:\n        full_path = os.path.join(dataset_dir, fname)\n        structures.append(Structure.from_file(full_path))\n        filenames.append(fname)\n\n    id_to_fname = {id(s): fn for s, fn in zip(structures, filenames)}\n\n    matcher = StructureMatcher()\n    grouped = matcher.group_structures(structures)\n\n    grouped_fnames = [[id_to_fname[id(s)] for s in group] for group in grouped]\n\n    if delete_duplicates:\n        for file_group in grouped_fnames:\n            # keep the first file, delete the rest\n            for dup_fname in file_group[1:]:\n                os.remove(os.path.join(dataset_dir, dup_fname))\n\n    return grouped_fnames\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v052-2025-08-13","title":"v0.5.2 - 2025-08-13","text":"<ul> <li>Reset verbosity level by @jcwang587 in #110</li> <li>Add a pre-commit hook for <code>pyproject</code> formatting by @jcwang587 in #111</li> <li>Add a pre-commit hook for <code>yaml</code> formatting by @jcwang587 in #112</li> <li>Correct documentation math expressions formatting by @jcwang587 in #113</li> <li>Add docstrings by @coderabbitai[bot] in #115</li> <li>Simplify <code>super()</code> calls by @jcwang587 in #117</li> <li>Add support returning plot axes by @jcwang587 in #119</li> <li>Correct import statements in example notebooks and documentation by @tlp-tau in #121</li> </ul>"},{"location":"changelog/#new-contributors","title":"New Contributors","text":"<ul> <li>@coderabbitai[bot] made their first contribution in #115</li> <li>@tlp-tau made their first contribution in #121</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.5.1...v0.5.2</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v051-2025-07-25","title":"v0.5.1 - 2025-07-25","text":"<ul> <li>Add function for generating convergence plots by @jcwang587 in #104</li> <li>Apply <code>np.mean()</code> for MAE, MSE, and RMSE computations by @jcwang587 in #106</li> <li>Add an optional parameter to scale metric values by @jcwang587 in #107</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.5.0...v0.5.1</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v050-2025-07-17","title":"v0.5.0 - 2025-07-17","text":"<ul> <li>pre-commit autoupdate by @pre-commit-ci[bot] in #92</li> <li>Add new scatter plot visualization by @jcwang587 in #93</li> <li>Plotting functions return the plot objects by @jcwang587 in #98</li> <li>Improve variable naming for axis limits by @jcwang587 in #99</li> </ul>"},{"location":"changelog/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@pre-commit-ci[bot] made their first contribution in #92</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.8...v0.5.0</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v048-2025-07-07","title":"v0.4.8 - 2025-07-07","text":"<ul> <li>Update GitHub Actions workflow to install the latest version of <code>uv</code> by @jcwang587 in #86</li> <li>Customizable units and improved formatting for hexbin plot by @jcwang587 in #87</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.7...v0.4.8</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v047-2025-06-29","title":"v0.4.7 - 2025-06-29","text":"<ul> <li>Enhance the pooling operation to use stacking instead of concatenation by @jcwang587 in #78</li> <li>Add <code>seed_everything</code> function by @jcwang587 in #79</li> <li>Generate a changelog by @jcwang587 in #80</li> <li>Update the <code>model-viewer</code> library by @jcwang587 in #81</li> <li>Add hexbin plot function and removed <code>pymatviz</code> from the required dependencies by @jcwang587 in #85</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.6...v0.4.7</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v046-2025-06-12","title":"v0.4.6 - 2025-06-12","text":"<ul> <li>Fixe an error when generating the parity plot for a specific range by @jcwang587 in #65</li> <li>Improve the <code>CIFData</code> caching mechanism by @jcwang587 in #76</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.5...v0.4.6</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v045-2025-06-08","title":"v0.4.5 - 2025-06-08","text":"<ul> <li>Add a new section for training options in the documentation by @jcwang587 in #46</li> <li>Info displayed with <code>logging</code> by @jcwang587 in #47</li> <li>Add flexible size of caching for DataLoader by @jcwang587 in #52</li> <li>Simplify dataset splitting process for training, validation, and testing by @jcwang587 in #54</li> <li>Enhance neighbor list loading with Cythonized <code>get_neighbor_list</code> by @jcwang587 in #60</li> <li>Change the output format of parity plots from SVG to PNG by @jcwang587 in #63</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.4...v0.4.5</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v044-2025-05-29","title":"v0.4.4 - 2025-05-29","text":"<ul> <li>Enhance model inference efficiency by adopting <code>torch.inference_mode</code> by @jcwang587 in #38</li> <li>Add option to force training set inclusion while not preserving the train:valid:test ratio by @jcwang587 in #39</li> <li>Add <code>print_checkpoint_info</code> by @jcwang587 in #40</li> <li>Set <code>shuffle=False</code> for validation and test set by @jcwang587 in #42</li> <li>Remove <code>scikit-learn</code> dependency by @jcwang587 in #44</li> <li>Switch the build system from <code>poetry</code> to <code>uv</code> for package building and publishing by @jcwang587 in #45</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.3...v0.4.4</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v043-2025-05-22","title":"v0.4.3 - 2025-05-22","text":"<ul> <li>Fix the error when inputting dataset without a split ratio by @jcwang587 in #36</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.2...v0.4.3</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v042-2025-05-16","title":"v0.4.2 - 2025-05-16","text":"<ul> <li>Allow custom x\u2011 and y\u2011axis labels in parity plot by @jcwang587 in #33</li> <li>Sort output indices by structure ID when running predictions by @jcwang587 in #34</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.1...v0.4.2</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v041-2025-04-29","title":"v0.4.1 - 2025-04-29","text":"<ul> <li>Add CLI scripts <code>id_gen</code> and <code>atom_gen</code> by @jcwang587 in #23</li> <li>Add documentation by @jcwang587 in #24</li> <li>Remove the requirement for <code>id_prop.csv</code> when running predictions on unknown datasets by @jcwang587 in #29</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.4.0...v0.4.1</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v040-2025-04-09","title":"v0.4.0 - 2025-04-09","text":"<ul> <li>Add a function to deduplicate CIF files for cleaning the dataset by @jcwang587 in #8</li> <li>Set up pre-trained models in subpackage by @jcwang587 in #10</li> <li>Add some missing type hints to <code>cgcnn_model.py</code> by @Andrew-S-Rosen in #14</li> <li>Fix return value in <code>p01_prediction.ipynb</code> by @Andrew-S-Rosen in #11</li> <li>Sort imports and remove unused imports by @Andrew-S-Rosen in #12</li> <li>Use <code>dict</code> instead of <code>Dict</code> for type hinting by @Andrew-S-Rosen in #13</li> <li>Fix docstring for <code>ConvLayer</code> by @Andrew-S-Rosen in #17</li> <li>Add a <code>cgcnn-pr</code> cli script for prediction by @jcwang587 in #20</li> <li>Add a <code>cgcnn-tr</code> cli script for training from scratch by @jcwang587 in #22</li> </ul>"},{"location":"changelog/#new-contributors_2","title":"New Contributors","text":"<ul> <li>@Andrew-S-Rosen made their first contribution in #14</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.3.4...v0.4.0</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v034-2025-03-26","title":"v0.3.4 - 2025-03-26","text":"<ul> <li>Add option to force training set inclusion while preserving the train:valid:test ratio by @jcwang587 in #7</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/compare/v0.3.2...v0.3.4</p> <p>Changes</p> <p></p>"},{"location":"changelog/#v032-2025-03-19","title":"v0.3.2 - 2025-03-19","text":"<ul> <li>Add an option to the fine-tuning script to generate a parity plot within a user-specified range</li> </ul>"},{"location":"changelog/#new-contributors_3","title":"New Contributors","text":"<ul> <li>@jcwang587 made their first contribution</li> </ul> <p>Full Changelog: https://github.com/jcwang587/cgcnn2/commits/v0.3.2</p> <p>Changes</p>"}]}